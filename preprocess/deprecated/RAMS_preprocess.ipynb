{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import networkx as nx\n",
    "import hypernetx as hnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from refined.inference.processor import Refined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined = Refined.from_pretrained(model_name='wikipedia_model_with_numbers',\n",
    "                                  entity_set=\"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_reader = jsonlines.open(r'data/raw/RAMS/dev.jsonlines')\n",
    "test_reader = jsonlines.open(r'data/raw/RAMS/test.jsonlines')\n",
    "train_reader = jsonlines.open(r'data/raw/RAMS/train.jsonlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_event_span(link, start_span, end_span):\n",
    "    trigger_span = link[0] # a list of [start, end]\n",
    "    if trigger_span[0] < start_span:\n",
    "        start_span = trigger_span[0]\n",
    "    if trigger_span[1] > end_span:\n",
    "        end_span = trigger_span[1]\n",
    "    argument_span = link[1] # a list of [start, end]\n",
    "    if argument_span[0] < start_span:\n",
    "        start_span = argument_span[0]\n",
    "    if argument_span[1] > end_span:\n",
    "        end_span = argument_span[1]\n",
    "    return start_span, end_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(datum):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in datum['sentences']] # merge the words into sentences\n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph\n",
    "\n",
    "def merge_events(datum):\n",
    "    words_flattened = [word for sentence in datum['sentences'] for word in sentence]\n",
    "    triggers = datum['evt_triggers']\n",
    "    trigger_type_dict = {}\n",
    "    for trigger_datum in triggers:\n",
    "        trigger_span = trigger_datum[:2]\n",
    "        trigger_word = \" \".join(words_flattened[trigger_span[0]:trigger_span[1]+1])\n",
    "        trigger_type = trigger_datum[2][0][0]\n",
    "        trigger_type_dict[trigger_word] = trigger_type\n",
    "    links = datum['gold_evt_links']\n",
    "    events = {}\n",
    "    start_span = 0\n",
    "    end_span = 0\n",
    "    for link in links:\n",
    "        trigger_span = link[0] # a list of [start, end]\n",
    "        trigger_word = \" \".join(words_flattened[trigger_span[0]:trigger_span[1]+1]) # a string\n",
    "        argument_span = link[1] # a list of [start, end]\n",
    "        argument_word = \" \".join(words_flattened[argument_span[0]:argument_span[1]+1]) # a string\n",
    "        argument_role = link[2] # a string\n",
    "        trigger_type = trigger_type_dict[trigger_word]\n",
    "        if trigger_word not in events.keys():\n",
    "            events[trigger_word] = {\n",
    "                \"trigger\": trigger_word,\n",
    "                \"trigger_span\": trigger_span,\n",
    "                \"trigger_type\": trigger_type,\n",
    "                \"arguments\": [\n",
    "                    {\n",
    "                        \"argument_id\": argument_word,\n",
    "                        \"argument_word\": argument_word,\n",
    "                        \"argument_role\": argument_role,\n",
    "                        \"argument_span\": argument_span\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        else:\n",
    "            events[trigger_word]['arguments'].append({\n",
    "                \"argument_id\": argument_word,\n",
    "                \"argument_word\": argument_word,\n",
    "                \"argument_role\": argument_role,\n",
    "                \"argument_span\": argument_span\n",
    "            })\n",
    "        # update event span\n",
    "        start_span, end_span = update_event_span(link, start_span, end_span)\n",
    "        events[trigger_word]['paragraph'] = datum['sentences']\n",
    "    return list(events.values())\n",
    "\n",
    "def link_entities(events, paragraph):\n",
    "    spans = refined.process_text(paragraph)\n",
    "    for span in spans:\n",
    "        entity_word = span.text\n",
    "        for event in events:\n",
    "            for argument in event['arguments']:\n",
    "                if argument['argument_word'] == entity_word:\n",
    "                    if span.predicted_entity != None and span.predicted_entity.wikidata_entity_id != None:\n",
    "                        entity_id = span.predicted_entity.wikidata_entity_id\n",
    "                        entity_title = span.predicted_entity.wikipedia_entity_title\n",
    "                        argument['argument_id'] = entity_id\n",
    "                        argument['entity_title'] = entity_title\n",
    "                    argument['entity_type'] = span.coarse_mention_type\n",
    "    return events\n",
    "\n",
    "def transform_dataset(dataset):\n",
    "    transformed_dataset = {}\n",
    "    for index, datum in enumerate(dataset):\n",
    "        print(\"{}/{}\".format(index, len(dataset)))\n",
    "        paragraph = merge_sentences(datum)\n",
    "        events = merge_events(datum)\n",
    "        if events == []: continue\n",
    "        events = link_entities(events, paragraph)\n",
    "        doc_key = datum['doc_key']\n",
    "        source_url = datum['source_url']\n",
    "        if doc_key not in transformed_dataset.keys():\n",
    "            transformed_dataset[doc_key] = {\n",
    "                \"doc_id\": doc_key,\n",
    "                \"source_url\": source_url,\n",
    "                \"events\": []\n",
    "            }\n",
    "        transformed_dataset[doc_key]['events'] += events\n",
    "    return list(transformed_dataset.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_reader = jsonlines.open(r'data/raw/RAMS/dev.jsonlines')\n",
    "for datum in dev_reader:\n",
    "    doc_id = datum['doc_key']\n",
    "    if doc_id == \"nw_RC0da9ca01673da1e2a47f6ccf9d239cbde98f30122f50c5ced8fa4743\":\n",
    "        paragraph = merge_sentences(datum)\n",
    "        events = merge_events(datum)\n",
    "        pprint(events)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = [datum for datum in dev_reader] + [datum for datum in test_reader] + [datum for datum in train_reader]\n",
    "dataset = [datum for datum in dev_reader]\n",
    "transformed_dataset = transform_dataset(dataset)\n",
    "save_json(transformed_dataset, r'data/result/RAMS/events.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate(docs):\n",
    "    nodes_dict = {}\n",
    "    hyper_edges_dict = {}\n",
    "    links = []\n",
    "    for doc in docs:\n",
    "        doc_id = doc['doc_id']\n",
    "        doc_url = doc['source_url']\n",
    "        for event in doc['events']:\n",
    "            arguments = event['arguments']\n",
    "            # create an entity node for each argument\n",
    "            for argument in arguments:\n",
    "                argument_id = argument['argument_id']\n",
    "                argument_word = argument['argument_word']\n",
    "                argument_title = argument['entity_title'] if 'entity_title' in argument else argument_word\n",
    "                argument_entity_type = argument['entity_type'] if 'entity_type' in argument else \"None\"\n",
    "                argument_span = argument['argument_span']\n",
    "                argument_role = argument['argument_role']\n",
    "                if argument_id not in nodes_dict.keys():\n",
    "                    nodes_dict[argument_id] = {\n",
    "                        \"id\": argument_id, \n",
    "                        \"title\": argument_title,\n",
    "                        \"entity_type\": argument_entity_type,\n",
    "                        \"type\": \"entity\",\n",
    "                        \"argument_role\": argument_role,\n",
    "                        \"mentions\": [\n",
    "                            {\n",
    "                                \"doc_id\": doc_id,\n",
    "                                \"mention\": argument_word,\n",
    "                                \"span\": {'start': argument_span[0], 'end': argument_span[1]}\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                else:\n",
    "                    nodes_dict[argument_id]['mentions'].append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"mention\": argument_word,\n",
    "                            \"span\": {'start': argument_span[0], 'end': argument_span[1]}\n",
    "                        }\n",
    "                    )\n",
    "            argument_ids = [argument['argument_id'] for argument in arguments]\n",
    "            if any([argument_id == None for argument_id in argument_ids]):\n",
    "                print(doc_id, argument_ids)\n",
    "            sorted_argument_ids = sorted(argument_ids)\n",
    "            # create hyperedge \n",
    "            trigger_id = event['trigger'] \n",
    "            trigger_type = event['trigger_type']\n",
    "            hyper_edge_id = trigger_id  + \"-\" + \"-\".join(sorted_argument_ids)\n",
    "            if hyper_edge_id not in hyper_edges_dict.keys():\n",
    "                hyper_edges_dict[hyper_edge_id] = {\n",
    "                    'id': hyper_edge_id,\n",
    "                    'type': \"hyper_edge\",\n",
    "                    \"trigger\": trigger_id,\n",
    "                    \"trigger_type\": trigger_type,\n",
    "                    \"arguments\": sorted_argument_ids,\n",
    "                    \"mentions\": [\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"paragraph\": event['paragraph']\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            else:\n",
    "                hyper_edges_dict[hyper_edge_id]['mentions'].append(\n",
    "                    {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"paragraph\": event['paragraph']\n",
    "                    }\n",
    "                )\n",
    "            for argument_id in argument_ids:\n",
    "                links.append((hyper_edge_id, argument_id))\n",
    "    return nodes_dict, hyper_edges_dict, links\n",
    "\n",
    "def merge_RAMS(dataset):\n",
    "    nodes_dict, hyper_edges_dict, links = disambiguate(dataset)\n",
    "    B = nx.Graph()\n",
    "    B.add_nodes_from(list(hyper_edges_dict.keys()), bipartite=0)\n",
    "    B.add_nodes_from(list(nodes_dict.keys()), bipartite=1)\n",
    "    B.add_edges_from(links)\n",
    "    return B, nodes_dict, hyper_edges_dict, links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, nodes_dict, hyper_edges_dict, links = merge_RAMS(transformed_dataset)\n",
    "\n",
    "H = hnx.Hypergraph.from_bipartite(B)\n",
    "list(H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_hgraph_data = nx.node_link_data(B)\n",
    "save_json(event_hgraph_data, r'data/result/RAMS/biHgraph_dev/hgraph.json')\n",
    "save_json(nodes_dict, r'data/result/RAMS/biHgraph_dev/nodes.json')\n",
    "save_json(hyper_edges_dict, r'data/result/RAMS/biHgraph_dev/hyperedges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_distribution(HG, fit_line=False):\n",
    "    degree_sequence = [HG.degree(node) for node in HG.nodes]\n",
    "    degree_counts = [(degree, degree_sequence.count(degree)) for degree in set(degree_sequence)]\n",
    "    x, y = zip(*degree_counts)\n",
    "        \n",
    "    # fit line\n",
    "    if fit_line:\n",
    "        filter_degree = 15\n",
    "        filtered_degree_sequence = list(filter(lambda degree: degree < filter_degree, degree_sequence))\n",
    "        filtered_degree_counts = [(degree, degree_sequence.count(degree)) for degree in set(filtered_degree_sequence)]\n",
    "        filtered_x, filtered_y = zip(*filtered_degree_counts)\n",
    "        log_x = np.log10(filtered_x)\n",
    "        log_y = np.log10(filtered_y)\n",
    "        slope, intercept = np.polyfit(log_x, log_y, 1)\n",
    "        print(\"slope:\", slope, \"intercept:\", intercept)\n",
    "        x_vals = np.array([min(filtered_x), max(filtered_x)])\n",
    "        y_vals = 10**(intercept + slope*np.log10(x_vals))\n",
    "        plt.plot(x_vals, y_vals, '--')\n",
    "    \n",
    "        \n",
    "    plt.scatter(x, y)\n",
    "    # plt.xscale(\"log\")\n",
    "    # plt.yscale(\"log\")\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(H.nodes))\n",
    "removed_nodes = [node for node in H.nodes if H.degree(node) == 1]\n",
    "SH = H.remove_nodes(removed_nodes)\n",
    "print(SH.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_frontend(nodes, links, nodes_dict, hyper_edges_dict):\n",
    "    res_nodes = []\n",
    "    res_links = []\n",
    "    for node in nodes:\n",
    "        if node in nodes_dict:\n",
    "            res_nodes.append(nodes_dict[node])\n",
    "        else:\n",
    "            res_nodes.append(hyper_edges_dict[node])\n",
    "    for link in links:\n",
    "        source = link[0]\n",
    "        target = link[1]\n",
    "        res_links.append({\n",
    "            \"source\": source,\n",
    "            \"target\": target,\n",
    "        })\n",
    "    print(len(res_nodes))\n",
    "    return {\n",
    "        \"nodes\": res_nodes, \n",
    "        \"links\": res_links\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BH = H.bipartite()\n",
    "network = transform_frontend(list(BH.nodes), list(BH.edges), nodes_dict, hyper_edges_dict)\n",
    "save_json(network, 'data/result/RAMS/dev_subgraph.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_hgraph_data = nx.node_link_data(BH)\n",
    "save_json(event_hgraph_data, r'event_network_data.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_hgraph_preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
