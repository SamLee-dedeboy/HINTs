{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "\n",
    "# From Sam: \n",
    "# I believe you have a different query approach. Feel free to switch to yours.\n",
    "def request_chatgpt_gpt4(messages):\n",
    "    url = \"http://127.0.0.1:5000/event_hgraph\"\n",
    "    body = {\"messages\": messages}\n",
    "    response = requests.post(url, json=body).json()\n",
    "    gpt_response = response['choices'][0]['message']['content'].strip()\n",
    "    return gpt_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the prompts to generate topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterLabelToHyperedge(cluster_labels, partition, hyperedge_dict):\n",
    "    reverse_partition = defaultdict(list)\n",
    "    for node_id, cluster_label in partition.items():\n",
    "        reverse_partition[str(cluster_label)].append(node_id)\n",
    "    hyperedges = []\n",
    "    for cluster_label in cluster_labels:\n",
    "        for hyperedge_id in reverse_partition[cluster_label.split(\"-\")[2]]:\n",
    "            hyperedges.append(hyperedge_dict[hyperedge_id])\n",
    "\n",
    "    return hyperedges\n",
    "\n",
    "def query_leaf_topic(hyperedges):\n",
    "    example = json.load(open(r'data/result/AllTheNews/cluster_summary/example.json'))\n",
    "    summaries = [hyperedge['summary'] for hyperedge in hyperedges]\n",
    "    summaries_message = \"\"\n",
    "    for index, summary in enumerate(summaries):\n",
    "        summaries_message += \"Article {}: \\n\".format(index+1)\n",
    "        summaries_message += summary + '\\n\\n\\n'\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a news article summarization system. \n",
    "                The user will provide you with a set of summarized news articles, your job is to further summarize them into one noun phrase.\n",
    "                Use words that are already in the articles, and try to use as few words as possible.\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": example['leaf']['summaries']},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": example['leaf']['topic']},\n",
    "        { \"role\": \"user\", \"content\": summaries_message}\n",
    "    ]\n",
    "    topic = request_chatgpt_gpt4(messages)\n",
    "    return topic\n",
    "\n",
    "def query_cluster_topic(cluster_subtopics, cluster_samples):\n",
    "    example = json.load(open(r'data/result/AllTheNews/cluster_summary/example.json'))\n",
    "    query = \"Sub-topics: \"\n",
    "    sample_summaries = \"\"\n",
    "    query += \", \".join(cluster_subtopics) + '\\n\\n\\n'\n",
    "    for index, cluster_sample in enumerate(cluster_samples):\n",
    "        sample_summaries += \"Article {}: \\n\".format(index+1)\n",
    "        sample_summaries += cluster_sample['summary'] + '\\n\\n\\n'\n",
    "\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a news article categorization system. \n",
    "                The user will provide you with a list of sub-topics of news articles and a few examples from the sub-topics.\n",
    "                Your job is to further categorize the sub-topics into a single noun-phrase that best summarizes all the sub-topics.\n",
    "                Try to reuse the words in the examples.\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": example['non-leaf']['summaries']},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": example['non-leaf']['topic']},\n",
    "        { \"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    topic = request_chatgpt_gpt4(messages)\n",
    "    return topic\n",
    "\n",
    "def add_hierarchical_topic(hierarchy, partitions, hyperedge_dict, topic_dict):\n",
    "    dfs(hierarchy, partitions, hyperedge_dict, topic_dict)\n",
    "    return topic_dict\n",
    "\n",
    "def dfs(hierarchy, partitions, hyperedge_dict, topic_dict):\n",
    "    level = int(hierarchy['key'].split('-')[1])\n",
    "    if level == 1:\n",
    "        children_labels = list(map(lambda x: x['key'], hierarchy['children']))\n",
    "        hyperedges = clusterLabelToHyperedge(children_labels, partitions[0], hyperedge_dict)\n",
    "        if hierarchy['key'] in topic_dict: return\n",
    "        gpt_topic = query_leaf_topic(hyperedges)\n",
    "        topic_dict[hierarchy['key']] = gpt_topic\n",
    "        save_json(topic_dict, 'data/result/AllTheNews/hierarchical_topics.json')\n",
    "        print(hierarchy['key'], gpt_topic)\n",
    "        return\n",
    "    else:\n",
    "        sub_topic_samples = []\n",
    "        all_hyperedges = []\n",
    "        for child in hierarchy['children']:\n",
    "            dfs(child, partitions, hyperedge_dict, topic_dict)\n",
    "            level = int(child['key'].split('-')[1])\n",
    "            hyperedges = clusterLabelToHyperedge([child['key']], partitions[level], hyperedge_dict)\n",
    "            sample = hyperedges[0]\n",
    "            sub_topic_samples.append(sample)\n",
    "            all_hyperedges += hyperedges\n",
    "        cluster_subtopics = [topic_dict[child['key']] for child in hierarchy['children']]\n",
    "        sample_hyperedges = random.sample(all_hyperedges, min(20, len(all_hyperedges)))\n",
    "        gpt_topic = query_cluster_topic(cluster_subtopics, sample_hyperedges)\n",
    "        # if hierarchy['key'] in topic_dict: return\n",
    "        # gpt_topic = query_leaf_topic(sub_topic_samples)\n",
    "        topic_dict[hierarchy['key']] = gpt_topic\n",
    "        save_json(topic_dict, 'data/result/AllTheNews/hierarchical_topics.json')\n",
    "        print(hierarchy['key'], gpt_topic)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is where the main function begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in hierarchy and partition\n",
    "hierarchy = json.load(open('data/result/AllTheNews/network/server/ravasz_hierarchies_entity.json'))\n",
    "partitions = json.load(open('data/result/AllTheNews/network/server/ravasz_partitions_entity.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read in hyperedges\n",
    "hyperedges_dict = json.load(open('data/result/AllTheNews/network/articles.json'))\n",
    "# 3. generate topic. hierarchical_topics.json should be empty at first\n",
    "topic_dict = json.load(open('data/result/AllTheNews/hierarchical_topics.json'))\n",
    "topic_dict = add_hierarchical_topic(hierarchy, partitions, hyperedges_dict, topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are testing/debugging functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = defaultdict(list)\n",
    "for node_id, cluster_id in partitions[1].items():\n",
    "    clusters[cluster_id].append(node_id)\n",
    "hyperedges_2344 = [hyperedges_dict[hyperedge_id] for hyperedge_id in clusters[2344]]\n",
    "query_leaf_topic(hyperedges_2344)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cluster = hierarchy['children'][0]['children'][0]['children'][0]['children'][0]['children'][1]\n",
    "cluster_children = [child['key'] for child in target_cluster['children']]\n",
    "sub_topic_samples = []\n",
    "topic_dict = {}\n",
    "for child in target_cluster['children']:\n",
    "    level = int(child['key'].split('-')[1])\n",
    "    hyperedges = clusterLabelToHyperedge([child['key']], partitions[level], hyperedges_dict)\n",
    "    print(len(hyperedges), len(child['children']))\n",
    "    sub_topic = query_leaf_topic(hyperedges)\n",
    "    topic_dict[child['key']] = sub_topic\n",
    "    sample = hyperedges[0]\n",
    "    sub_topic_samples.append(sample)\n",
    "pprint(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = [752, 1069, 1070, 1478]\n",
    "# cluster_subtopics = [\n",
    "#     'Increasing Gun Violence in Chicago',\n",
    "#     'Crime Rates and Policing Tactics',\n",
    "#     'Misconceptions about Crime in the United States',\n",
    "#     'Global Events and Optimism',\n",
    "# ]\n",
    "cluster_subtopics = [topic_dict[\"L-1-{}\".format(cluster_label)] for cluster_label in children]\n",
    "cluster_samples = []\n",
    "clusters = defaultdict(list)\n",
    "for node_id, cluster_label in partitions[1].items():\n",
    "    clusters[cluster_label].append(node_id)\n",
    "for cluster_label in children:\n",
    "    cluster_samples += clusters[cluster_label]\n",
    "cluster_samples = random.sample(cluster_samples, 10)\n",
    "cluster_samples = [hyperedges_dict[sample] for sample in cluster_samples]\n",
    "topic = query_cluster_topic(cluster_subtopics, cluster_samples)\n",
    "sample_summaries = [sample['summary'] for sample in cluster_samples]\n",
    "print(topic)\n",
    "pprint(cluster_subtopics)\n",
    "for summary in sample_summaries:\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for generating few-show examples for the prompt\n",
    "sample_summaries = \"Sub-topics: \"\n",
    "sample_summaries += \", \".join(cluster_subtopics) + '\\n\\n\\n'\n",
    "for index, cluster_sample in enumerate(cluster_samples):\n",
    "    sample_summaries += \"Article {}: \\n\".format(index+1)\n",
    "    sample_summaries += cluster_sample['summary'] + '\\n\\n\\n'\n",
    "example = json.load(open(r'data/result/AllTheNews/cluster_summary/example.json'))\n",
    "example['non-leaf']['summaries'] = sample_summaries\n",
    "example['non-leaf']['topic'] = 'Crimes in the United States'\n",
    "save_json(example, r'data/result/AllTheNews/cluster_summary/example.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_hgraph_preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
