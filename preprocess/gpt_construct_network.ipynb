{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No module named 'celluloid'. If you need to use hypernetx.algorithms.contagion, please install additional packages by running the following command: pip install .['all']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import networkx as nx\n",
    "import hypernetx as hnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "# from refined.inference.processor import Refined\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined = Refined.from_pretrained(model_name='wikipedia_model_with_numbers',\n",
    "                                  entity_set=\"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2632\n",
      "['interaction', 'data visualization', 'accessibility']\n",
      "['visual analysis', 'cluster analysis']\n",
      "['data transformation', 'declarative specification']\n",
      "['aesthetics']\n",
      "['visual analytics', 'deep learning', 'explainability']\n",
      "['visual analytics', 'design study', 'theory', 'qualitative study', 'application']\n",
      "['uncertainty', 'bar charts', 'progressive visualization']\n",
      "['tabular data', 'data transformation', 'tabular visualization']\n",
      "['time-series visualization', 'ensemble learning']\n",
      "['interaction', 'data visualization', 'aesthetics']\n",
      "['software visualization', 'performance analysis', 'parallel computing', 'traces', 'event sequence visualization']\n",
      "['storytelling', 'diversity']\n",
      "['scalar field visualization', 'pixel-based visualization']\n",
      "['scientific visualization', 'volume visualization', 'ray casting', 'gaussian mixture models']\n",
      "['perception', 'data visualization', 'visual analysis', 'cognition', 'data interpretation']\n",
      "['feature detection', 'scalar field data', 'physical & environmental sciences', 'mathematics']\n",
      "['human-centered computing', 'visualization design and evaluation methods']\n",
      "['molecular dynamics', 'structure', 'progressive analytics']\n",
      "['sports visualization']\n",
      "['data analysis', 'data storytelling']\n",
      "['visualization', 'design', 'annotation', 'line charts', 'text']\n",
      "['graph layout', 'framework']\n",
      "['multivariate data']\n",
      "['visualization']\n",
      "['natural language interface', 'visualization authoring']\n",
      "['data transformation', 'program understanding', 'table comparison']\n",
      "['interactive exploration', 'immersive visualization', 'cultural heritage']\n",
      "['visual analytics', 'training']\n",
      "['scalar fields', 'persistence', 'merge trees', 'stability']\n",
      "['genomics', 'responsive visualization', 'visualization grammar', 'multi-view visualization']\n",
      "['explainable ai', 'interactive visual analytics']\n",
      "['data visualization', 'sports analytics']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'keyword'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m articles \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdata/raw/VisPub/articles_w_keywords.json\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(articles))\n\u001b[0;32m---> 29\u001b[0m transformed \u001b[39m=\u001b[39m transform_vispub(articles)\n\u001b[1;32m     30\u001b[0m \u001b[39m# save_json(transformed, 'data/result/VisPub/linked/linked_1010.json')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mtransform_vispub\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m      2\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m articles:\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mprint\u001b[39m(article[\u001b[39m'\u001b[39;49m\u001b[39mkeyword\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      5\u001b[0m     res\u001b[39m.\u001b[39mappend({\n\u001b[1;32m      6\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: article[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m\"\u001b[39m: article[\u001b[39m'\u001b[39m\u001b[39mAbstract\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         }\n\u001b[1;32m     25\u001b[0m     })\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "\u001b[0;31mKeyError\u001b[0m: 'keyword'"
     ]
    }
   ],
   "source": [
    "def transform_vispub(articles):\n",
    "    res = []\n",
    "    for article in articles:\n",
    "        print(article['keyword'])\n",
    "        res.append({\n",
    "            \"id\": article['id'],\n",
    "            \"summary\": article['Abstract'],\n",
    "            \"content\": article['Abstract'],\n",
    "            \"title\": article['Title'],\n",
    "            \"date\": article['Year'],\n",
    "            \"publication\": article['Conference'],\n",
    "            \"event\": {\n",
    "                \"title\": article['Title'],\n",
    "                \"type\": \"publication\",\n",
    "                \"participants\": [\n",
    "                    {\n",
    "                        \"entity_id\": keyword,\n",
    "                        \"entity_title\": keyword,\n",
    "                        \"raw_mention\": keyword,\n",
    "                        \"entity_type\": None\n",
    "                    }\n",
    "                    for keyword in article['keyword']\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "    return res\n",
    "articles = json.load(open('data/raw/VisPub/articles_w_keywords.json'))\n",
    "print(len(articles))\n",
    "transformed = transform_vispub(articles)\n",
    "# save_json(transformed, 'data/result/VisPub/linked/linked_1010.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(sentences):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in sentences] # merge the words into sentences\n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph\n",
    "\n",
    "def prepare_events(datum):\n",
    "    return datum['events']\n",
    "    # words_flattened = [word for sentence in datum['sentences'] for word in sentence]\n",
    "    for event in datum['events']:\n",
    "        # trigger = event['trigger']\n",
    "        arguments = event['arguments']\n",
    "        arguments_obj = [\n",
    "            { \n",
    "                'argument_id': argument, \n",
    "                'argument_word': argument,\n",
    "            }\n",
    "            for argument in arguments\n",
    "            ]\n",
    "        event['arguments'] = arguments_obj\n",
    "    return datum['events']\n",
    "\n",
    "def link_entities(event, paragraph):\n",
    "    # spans = refined.process_text(paragraph)\n",
    "    res = []\n",
    "    for participant in event['participants']:\n",
    "        spans = refined.process_text(participant)\n",
    "        if spans == []: \n",
    "            participant = {\n",
    "                \"entity_id\": participant,\n",
    "                \"entity_title\": participant,\n",
    "                \"raw_mention\": participant,\n",
    "                \"entity_type\": \"None\"\n",
    "            }\n",
    "            res.append(participant)\n",
    "            continue\n",
    "        span = [span for span in spans][0]\n",
    "        entity_word = span.text\n",
    "        print(span)\n",
    "        if span.predicted_entity != None and span.predicted_entity.wikidata_entity_id != None:\n",
    "            participant = {\n",
    "                \"entity_id\": span.predicted_entity.wikidata_entity_id,\n",
    "                \"entity_title\": span.predicted_entity.wikipedia_entity_title,\n",
    "                \"raw_mention\": participant,\n",
    "                \"entity_type\": span.coarse_mention_type or \"None\",\n",
    "            }\n",
    "        else:\n",
    "            participant = {\n",
    "                \"entity_id\": participant,\n",
    "                \"entity_title\": participant,\n",
    "                \"raw_mention\": participant,\n",
    "                \"entity_type\": span.coarse_mention_type or \"None\",\n",
    "            }\n",
    "        res.append(participant)\n",
    "    event['participants'] = res\n",
    "    return event\n",
    "\n",
    "    for span in spans:\n",
    "        entity_word = span.text\n",
    "        for event in events:\n",
    "            for argument in event['arguments']:\n",
    "                if argument['argument_word'] == entity_word:\n",
    "                    if span.predicted_entity != None and span.predicted_entity.wikidata_entity_id != None:\n",
    "                        entity_id = span.predicted_entity.wikidata_entity_id\n",
    "                        entity_title = span.predicted_entity.wikipedia_entity_title\n",
    "                        argument['argument_id'] = entity_id\n",
    "                        argument['entity_title'] = entity_title\n",
    "                    argument['entity_type'] = span.coarse_mention_type \n",
    "    return events\n",
    "\n",
    "def transform_dataset(dataset):\n",
    "    transformed_dataset = {}\n",
    "    for index, datum in enumerate(dataset):\n",
    "        print(\"{}/{}\".format(index, len(dataset)))\n",
    "        paragraph = datum['summary']\n",
    "        event = prepare_events(datum)\n",
    "        event = link_entities(event, paragraph)\n",
    "        doc_key = datum['id']\n",
    "        source_url = datum['url']\n",
    "        if doc_key not in transformed_dataset.keys():\n",
    "            datum['events'] = event\n",
    "            transformed_dataset[doc_key] = datum\n",
    "    return list(transformed_dataset.values())\n",
    "\n",
    "def remove_duplicates(dataset):\n",
    "    reverse_index_url = defaultdict(list)\n",
    "    kept_dataset = []\n",
    "    for index, datum in enumerate(dataset):\n",
    "        if datum['source_url'] in reverse_index_url.keys():\n",
    "            previous_data = reverse_index_url[datum['source_url']]\n",
    "            duplicate = False\n",
    "            for previous_datum in previous_data:\n",
    "                if \" \".join(datum['content'][0][0]) == \" \".join(previous_datum['content'][0][0]):\n",
    "                    duplicate = True\n",
    "                    break\n",
    "            if duplicate: continue\n",
    "        reverse_index_url[datum['source_url']].append(datum)\n",
    "        kept_dataset.append(datum)\n",
    "    return kept_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_participant_mentions(participants, paragraph):\n",
    "    spans = refined.process_text(paragraph)\n",
    "    print(spans)\n",
    "    # participant_ids = [participant['entity_id'] for participant in participants]\n",
    "    mentioned_participants = []\n",
    "    mention_texts = []\n",
    "    for participant in participants:\n",
    "        if participant['entity_id'] == participant['entity_title']:\n",
    "            mentioned_participants.append({\n",
    "                'text': participant['raw_mention'],\n",
    "                'entity_id': participant['entity_id'],\n",
    "                'entity_title': participant['entity_title'],\n",
    "            })\n",
    "        else:\n",
    "            find = False\n",
    "            for span in spans:\n",
    "                if span.predicted_entity != None and span.predicted_entity.wikidata_entity_id != None:\n",
    "                    entity_id = span.predicted_entity.wikidata_entity_id\n",
    "                    entity_title = span.predicted_entity.wikipedia_entity_title\n",
    "                    mention = span.text\n",
    "                    if entity_id == participant['entity_id'] and mention not in mention_texts:\n",
    "                        find = True\n",
    "                        mention_texts.append(mention)\n",
    "                        mentioned_participants.append({\n",
    "                            'text': mention,\n",
    "                            'entity_id': entity_id,\n",
    "                            'entity_title': entity_title,\n",
    "                        })\n",
    "            if not find:\n",
    "                mentioned_participants.append({\n",
    "                    'text': participant['raw_mention'],\n",
    "                    'entity_id': participant['entity_id'],\n",
    "                    'entity_title': participant['entity_title'],\n",
    "                }) \n",
    "    # find mention spans\n",
    "    mentioned_participants = find_keyword_spans(paragraph, mentioned_participants)\n",
    "    return mentioned_participants\n",
    "def find_keyword_spans(paragraph, participants):\n",
    "    for participant in participants:\n",
    "        keyword_spans = []\n",
    "        keyword = participant['text']\n",
    "        if keyword.startswith(' ['):\n",
    "            keyword = keyword[2:]\n",
    "        pattern = re.compile(re.escape(keyword), re.IGNORECASE)\n",
    "        matches = pattern.finditer(paragraph)\n",
    "        for match in matches:\n",
    "            start_span = match.start()\n",
    "            end_span = match.end()\n",
    "            keyword_spans.append((start_span, end_span, keyword))\n",
    "        participant['spans'] = keyword_spans\n",
    "    return participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dict = json.load(open('data/result/AllTheNews/network/articles.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph = \"\"\"\n",
    "# The article discussed the brief interaction between President Obama and President Rodrigo Duterte at a summit meeting in Laos. After Duterte's profane outburst, Obama canceled their first meeting, but they spoke briefly at a gala dinner. The conversation was not substantive, and they were seated far apart during the dinner.\n",
    "# \"\"\"\n",
    "# find_participant_mentions([\"Q76\", \"Q457786\"], paragraph)\n",
    "# article_participant_span_dict = json.load(open('data/result/AllTheNews/network/server/article_participant_spans.json'))\n",
    "article_participant_span_dict = {}\n",
    "count = 0\n",
    "for article_id, article_data in article_dict.items():\n",
    "    print(\"{}/{}\".format(count, len(article_dict)))\n",
    "    count += 1\n",
    "    # if count > 50: break\n",
    "    paragraph = article_data['summary']\n",
    "    participants = [entity for entity in article_data['event']['participants']]\n",
    "    participant_spans = find_participant_mentions(participants, paragraph)\n",
    "    article_participant_span_dict[article_id] = participant_spans\n",
    "    # save_json(article_participant_span_dict, 'data/result/AllTheNews/network/article_participant_spans.json')\n",
    "save_json(article_participant_span_dict, 'data/result/AllTheNews/network/article_participant_spans.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_participant_span_dict = json.load(open('data/result/AllTheNews/network/article_participant_spans.json'))\n",
    "article_dict = json.load(open('data/result/AllTheNews/network/articles.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article_id, entities in article_participant_span_dict.items():\n",
    "    for entity in entities:\n",
    "        if len(entity['spans']) == 0:\n",
    "            print(article_id, entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_data = json.load(open(\"data/result/AllTheNews/linked/2016_10p_0819_2.json\"))\n",
    "linked_ids = [article['id'] for article in linked_data]\n",
    "old_linked_data = json.load(open(\"data/result/AllTheNews/linked/2016_10p.json\"))\n",
    "old_linked_ids = [article['doc_id'] for article in old_linked_data]\n",
    "target_data_ids = [old_article_id for old_article_id in old_linked_ids if old_article_id not in linked_ids]\n",
    "print(len(target_data_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_articles = json.load(open(\"data/raw/AllTheNews/events/2016_10p_0819.json\"))\n",
    "All_articles_dict = { article['id']: article for article in All_articles}\n",
    "target_data = [All_articles_dict[target_data_id] for target_data_id in target_data_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transformed_data = transform_dataset(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_data in new_transformed_data:\n",
    "    linked_data.append(new_data)\n",
    "save_json(linked_data, 'data/result/AllTheNews/linked/2016_10p_0819_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the News\n",
    "# AllTheNews = json.load(open(r'data/result/AllTheNews/preprocessed/2016_10p.json'))\n",
    "AllTheNews = json.load(open(r'data/raw/AllTheNews/events/2016_10p_0819.json'))\n",
    "transformed_dataset = transform_dataset(AllTheNews)\n",
    "print(len(AllTheNews), len(transformed_dataset))\n",
    "# save_json(transformed_dataset, r'data/result/AllTheNews/linked/2016_10p_0819.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(transformed_dataset, r'data/result/AllTheNews/linked/2016_10p_0819.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_dataset = json.load(open(r'data/result/AllTheNews/linked/2016_10p_0819.json'))\n",
    "for datum in prev_dataset:\n",
    "    if 'events' in datum.keys():\n",
    "        event = datum['events']\n",
    "        datum['event'] = datum['events']\n",
    "        del datum['events']\n",
    "save_json(prev_dataset, r'data/result/AllTheNews/linked/2016_10p_0819.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_round_linking(events):\n",
    "    for event in events:\n",
    "        for argument in event['arguments']:\n",
    "            if argument['argument_id'] == argument['argument_word']:\n",
    "                if 'entity_type' in argument.keys(): del argument['entity_type']\n",
    "                spans = refined.process_text(argument['argument_word'])\n",
    "                for span in spans:\n",
    "                    if span.predicted_entity != None and span.predicted_entity.wikidata_entity_id != None:\n",
    "                        entity_id = span.predicted_entity.wikidata_entity_id\n",
    "                        entity_title = span.predicted_entity.wikipedia_entity_title\n",
    "                        argument['argument_id'] = entity_id\n",
    "                        argument['entity_title'] = entity_title\n",
    "                        argument['entity_type'] = span.coarse_mention_type\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# create node link graph\n",
    "def construct_network(docs):\n",
    "    entity_dict = {}\n",
    "    article_dict = {}\n",
    "    links = []\n",
    "    for doc in docs:\n",
    "        doc_id = doc['id']\n",
    "        article_dict[doc_id] = doc\n",
    "        event = doc['event']\n",
    "        article_id = str(doc_id)\n",
    "        participants = event['participants']\n",
    "        # create an entity node for each participant\n",
    "        for participant in participants:\n",
    "            participant_id = participant['entity_id']\n",
    "            participant_word = participant['raw_mention']\n",
    "            participant_title = participant['entity_title'] \n",
    "            participant_entity_type = participant['entity_type'] \n",
    "\n",
    "            if participant_id not in entity_dict.keys():\n",
    "                entity_dict[participant_id] = {\n",
    "                    \"id\": participant_id, \n",
    "                    \"title\": participant_title,\n",
    "                    \"entity_type\": participant_entity_type,\n",
    "                    \"type\": \"entity\",\n",
    "                    \"mentions\": [\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"mention\": participant_word,\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            else:\n",
    "                entity_dict[participant_id]['mentions'].append(\n",
    "                    {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"mention\": participant_word,\n",
    "                    }\n",
    "                )\n",
    "            participant_ids = [participant['entity_id'] for participant in participants]\n",
    "            for participant in participants:\n",
    "                participant_id = participant['entity_id']\n",
    "                links.append((article_id, participant_id))\n",
    "\n",
    "    return entity_dict, article_dict, links\n",
    "\n",
    "def merge_network(dataset):\n",
    "    entity_dict, article_dict, links = construct_network(dataset)\n",
    "    B = nx.Graph()\n",
    "    B.add_nodes_from(list(article_dict.keys()), bipartite=0)\n",
    "    B.add_nodes_from(list(entity_dict.keys()), bipartite=1)\n",
    "    B.add_edges_from(links)\n",
    "\n",
    "    return B, entity_dict, article_dict, links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = json.load(open('data/result/AllTheNews/linked/2016_10p_0819.json'))\n",
    "for datum in transformed_dataset:\n",
    "    if \"events\" in datum.keys():\n",
    "        datum['event'] = datum['events']\n",
    "        del datum['events']\n",
    "save_json(transformed_dataset, 'data/result/AllTheNews/linked/2016_10p_0819.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_linked_data = json.load(open(\"data/result/AllTheNews/linked/2016_10p.json\"))\n",
    "old_linked_ids = [article['doc_id'] for article in old_linked_data]\n",
    "res = []\n",
    "for datum in transformed_dataset:\n",
    "    if datum[\"id\"] in old_linked_ids:\n",
    "        res.append(datum)\n",
    "save_json(res, 'data/result/AllTheNews/linked/2016_10p_0819_2.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_dataset = json.load(open('data/result/RAMS/gpt_events_dev_linked.json'))\n",
    "# transformed_dataset = json.load(open('data/result/AllTheNews/linked/2016_10p.json'))\n",
    "# transformed_dataset = json.load(open('data/result/AllTheNews/linked/2016_10p_0819.json'))\n",
    "transformed_dataset = json.load(open('data/result/VisPub/linked/linked.json'))\n",
    "B, entity_dict, article_dict, links = merge_network(transformed_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(transformed_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = hnx.Hypergraph.from_bipartite(B)\n",
    "list(H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_hgraph_data = nx.node_link_data(B)\n",
    "save_json(event_hgraph_data, r'data/result/VisPub/network/hgraph.json')\n",
    "save_json(entity_dict, r'data/result/VisPub/network/entities.json')\n",
    "save_json(article_dict, r'data/result/VisPub/network/articles.json')\n",
    "\n",
    "# save_json(sub_event_hyperedges, r'data/result/AllTheNews/sub_network/sub_event_hyperedges.json')\n",
    "# save_json(sub_event_links_dict, r'data/result/AllTheNews/sub_network/sub_event_links.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_frontend(nodes, links, nodes_dict, hyper_edges_dict):\n",
    "    res_nodes = []\n",
    "    res_links = []\n",
    "    for node in nodes:\n",
    "        if node in nodes_dict:\n",
    "            nodes_dict[node]['type'] = 'entity'\n",
    "            res_nodes.append(nodes_dict[node])\n",
    "        else:\n",
    "            print(hyper_edges_dict[node]['date'])\n",
    "            date = hyper_edges_dict[node]['date'].replace(\"-\", \"/\")\n",
    "            date.replace(\"-\", \"/\")\n",
    "            hyper_edges_dict[node]['date'] = date\n",
    "            hyper_edges_dict[node]['type'] = 'article'\n",
    "            print(hyper_edges_dict[node]['date'])\n",
    "\n",
    "            res_nodes.append(hyper_edges_dict[node])\n",
    "    for link in links:\n",
    "        source = link[0]\n",
    "        target = link[1]\n",
    "        res_links.append({\n",
    "            \"source\": source,\n",
    "            \"target\": target,\n",
    "        })\n",
    "    print(len(res_nodes))\n",
    "    return {\n",
    "        \"nodes\": res_nodes, \n",
    "        \"links\": res_links\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BH = H.bipartite()\n",
    "network = transform_frontend(list(BH.nodes), list(BH.edges), entity_dict, article_dict)\n",
    "# save_json(network, 'data/result/AllTheNews/network/server/frontend.json')\n",
    "save_json(network, 'data/result/VisPub/network/server/frontend.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = json.load(open('data/result/AllTheNews/network/server/frontend.json'))\n",
    "hyperedge_nodes = list(filter(lambda node: node['type'] == 'hyper_edge', network['nodes']))\n",
    "entity_nodes = list(filter(lambda node: node['type'] == 'entity' and node['id'] != node['title'], network['nodes']))\n",
    "\n",
    "entity_node_ids = list(map(lambda node: node['id'], entity_nodes))\n",
    "hyperedge_node_ids = list(map(lambda node: node['id'], hyperedge_nodes))\n",
    "entity_links = list(filter(lambda link: link['source'] in entity_node_ids or link['target'] in entity_node_ids, network['links']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = json.load(open('data/result/AllTheNews/network/server/ravasz_partitions_article.json'))\n",
    "frontend_data = json.load(open('data/result/AllTheNews/network/server/frontend.json'))\n",
    "nodes = frontend_data['nodes']\n",
    "article_nodes = [node for node in nodes if node['type'] == 'hyper_edge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_trigger_doc_id_dict = {}\n",
    "for article_node in article_nodes:\n",
    "    old_trigger_doc_id_dict[article_node['id']] = article_node['doc_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_frontend_data = json.load(open('data/result/AllTheNews/network/server/old/frontend.json'))\n",
    "old_article_nodes = [node for node in old_frontend_data['nodes'] if node['type'] == 'hyper_edge']\n",
    "old_article_doc_ids = [node['doc_id'] for node in old_article_nodes]\n",
    "print(len(old_article_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontend_data = json.load(open('data/result/AllTheNews/network/server/frontend_2.json'))\n",
    "article_nodes = [node for node in frontend_data['nodes'] if node['type'] == 'article']\n",
    "entity_nodes = [node for node in frontend_data['nodes'] if node['type'] == 'entity']\n",
    "print(len(article_nodes), len(entity_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = { article['id']: article for article in article_nodes }\n",
    "entities = { entity['id']: entity for entity in entity_nodes }\n",
    "save_json(articles, 'data/result/AllTheNews/network/articles.json')\n",
    "save_json(entities, 'data/result/AllTheNews/network/entities.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = frontend_data['links']\n",
    "filtered_links = []\n",
    "for link in links:\n",
    "    source = link['source']\n",
    "    target = link['target']\n",
    "    if (source in articles or source in entities) and (target in articles or target in entities):\n",
    "        filtered_links.append((source, target))\n",
    "print(len(filtered_links), len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nx.Graph()\n",
    "B.add_nodes_from(list(articles.keys()), bipartite=0)\n",
    "B.add_nodes_from(list(entities.keys()), bipartite=1)\n",
    "B.add_edges_from(filtered_links)\n",
    "event_hgraph_data = nx.node_link_data(B)\n",
    "save_json(event_hgraph_data, r'data/result/AllTheNews/network/hgraph.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.number_of_nodes(), len(articles), len(entities), len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in B.nodes():\n",
    "    if v not in articles and v not in entities:\n",
    "        print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_hgraph_preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
