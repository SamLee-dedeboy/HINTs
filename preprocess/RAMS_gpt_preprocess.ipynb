{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No module named 'celluloid'. If you need to use hypernetx.algorithms.contagion, please install additional packages by running the following command: pip install .['all']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samytlee/opt/anaconda3/envs/event_hgraph_preprocess/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import networkx as nx\n",
    "import hypernetx as hnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from refined.inference.processor import Refined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined = Refined.from_pretrained(model_name='wikipedia_model_with_numbers',\n",
    "                                  entity_set=\"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(sentences):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in sentences] # merge the words into sentences\n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph\n",
    "\n",
    "def prepare_events(datum):\n",
    "    # words_flattened = [word for sentence in datum['sentences'] for word in sentence]\n",
    "    for event in datum['events']:\n",
    "        # trigger = event['trigger']\n",
    "        arguments = event['arguments']\n",
    "        arguments_obj = [\n",
    "            { \n",
    "                'argument_id': argument, \n",
    "                'argument_word': argument,\n",
    "            }\n",
    "            for argument in arguments\n",
    "            ]\n",
    "        event['arguments'] = arguments_obj\n",
    "    return datum['events']\n",
    "\n",
    "def link_entities(events, paragraph):\n",
    "    spans = refined.process_text(paragraph)\n",
    "    for span in spans:\n",
    "        entity_word = span.text\n",
    "        for event in events:\n",
    "            for argument in event['arguments']:\n",
    "                if argument['argument_word'] == entity_word:\n",
    "                    if span.predicted_entity != None and span.predicted_entity.wikidata_entity_id != None:\n",
    "                        entity_id = span.predicted_entity.wikidata_entity_id\n",
    "                        entity_title = span.predicted_entity.wikipedia_entity_title\n",
    "                        argument['argument_id'] = entity_id\n",
    "                        argument['entity_title'] = entity_title\n",
    "                    argument['entity_type'] = span.coarse_mention_type\n",
    "    return events\n",
    "\n",
    "def transform_dataset(dataset):\n",
    "    transformed_dataset = {}\n",
    "    for index, datum in enumerate(dataset):\n",
    "        print(\"{}/{}\".format(index, len(dataset)))\n",
    "        paragraph = merge_sentences(datum['content'])\n",
    "        events = prepare_events(datum)\n",
    "        events = link_entities(events, paragraph)\n",
    "        if events == []: continue\n",
    "        doc_key = datum['doc_id']\n",
    "        source_url = datum['url']\n",
    "        if doc_key not in transformed_dataset.keys():\n",
    "            transformed_dataset[doc_key] = {\n",
    "                \"doc_id\": doc_key,\n",
    "                \"source_url\": source_url,\n",
    "                \"content\": datum['content'],\n",
    "                \"summary\": datum['summary'],\n",
    "                \"events\": []\n",
    "            }\n",
    "        transformed_dataset[doc_key]['events'] += events\n",
    "    return list(transformed_dataset.values())\n",
    "\n",
    "def remove_duplicates(dataset):\n",
    "    reverse_index_url = defaultdict(list)\n",
    "    kept_dataset = []\n",
    "    for index, datum in enumerate(dataset):\n",
    "        if datum['source_url'] in reverse_index_url.keys():\n",
    "            previous_data = reverse_index_url[datum['source_url']]\n",
    "            duplicate = False\n",
    "            for previous_datum in previous_data:\n",
    "                if \" \".join(datum['content'][0][0]) == \" \".join(previous_datum['content'][0][0]):\n",
    "                    duplicate = True\n",
    "                    break\n",
    "            if duplicate: continue\n",
    "        reverse_index_url[datum['source_url']].append(datum)\n",
    "        kept_dataset.append(datum)\n",
    "    return kept_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = [datum for datum in dev_reader] + [datum for datum in test_reader] + [datum for datum in train_reader]\n",
    "# dataset = [datum for datum in dev_reader]\n",
    "dataset = json.load(open('data/result/RAMS/gpt_events_dev.json'))\n",
    "transformed_dataset = transform_dataset(dataset)\n",
    "transformed_dataset = remove_duplicates(dataset)\n",
    "# second round linking\n",
    "for datum in dataset:\n",
    "    datum['events'] = second_round_linking(datum['events'])\n",
    "save_json(transformed_dataset, r'data/result/RAMS/gpt_events_dev_linked.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_round_linking(events):\n",
    "    for event in events:\n",
    "        for argument in event['arguments']:\n",
    "            if argument['argument_id'] == argument['argument_word']:\n",
    "                if 'entity_type' in argument.keys(): del argument['entity_type']\n",
    "                spans = refined.process_text(argument['argument_word'])\n",
    "                for span in spans:\n",
    "                    if span.predicted_entity != None and span.predicted_entity.wikidata_entity_id != None:\n",
    "                        entity_id = span.predicted_entity.wikidata_entity_id\n",
    "                        entity_title = span.predicted_entity.wikipedia_entity_title\n",
    "                        argument['argument_id'] = entity_id\n",
    "                        argument['entity_title'] = entity_title\n",
    "                        argument['entity_type'] = span.coarse_mention_type\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open('data/result/RAMS/gpt_events_dev_linked.json'))\n",
    "for datum in dataset:\n",
    "    datum['events'] = second_round_linking(datum['events'])\n",
    "save_json(dataset, r'data/result/RAMS/gpt_events_dev_linked.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# create node link graph\n",
    "def construct_network(docs):\n",
    "    nodes_dict = {}\n",
    "    event_hyperedges_dict = {}\n",
    "    links = []\n",
    "    sub_event_links = defaultdict(list)\n",
    "    sub_event_hyperedges_dict = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        doc_id = doc['doc_id']\n",
    "        doc_url = doc['source_url']\n",
    "        event_triggers = []\n",
    "        event_arguments = set()\n",
    "        event_triggers = sorted(list(set([sub_event['trigger'] for sub_event in doc['events']])))\n",
    "        # check gpt response for error\n",
    "        if len(event_triggers) == 1:\n",
    "            trigger = event_triggers[0]\n",
    "            if trigger.startswith(\"I'm sorry\"): continue\n",
    "            if trigger.startswith(\"No event\"): continue\n",
    "            if trigger.startswith(\"There are no\"): continue\n",
    "            if trigger.startswith(\"There is no\"): continue\n",
    "            if trigger.startswith(\"I'm unable to \"): continue\n",
    "\n",
    "        event_hyperedge_id = str(doc_id) +  \"-\" + \"-\".join(event_triggers)\n",
    "        for sub_event in doc['events']:\n",
    "            arguments = sub_event['arguments']\n",
    "            # create an entity node for each argument\n",
    "            for argument in arguments:\n",
    "                if argument['argument_id'] == argument['argument_word']: \n",
    "                    argument['argument_id'] = argument['argument_id'] + '-' + str(doc_id)\n",
    "                argument_id = argument['argument_id']\n",
    "                argument_word = argument['argument_word']\n",
    "                argument_title = argument['entity_title'] if 'entity_title' in argument else argument_word\n",
    "                argument_entity_type = argument['entity_type'] if 'entity_type' in argument else \"None\"\n",
    "                # argument_span = argument['argument_span']\n",
    "                # argument_role = argument['argument_role']\n",
    "                if argument_id not in nodes_dict.keys():\n",
    "                    nodes_dict[argument_id] = {\n",
    "                        \"id\": argument_id, \n",
    "                        \"title\": argument_title,\n",
    "                        \"entity_type\": argument_entity_type,\n",
    "                        \"type\": \"entity\",\n",
    "                        # \"argument_role\": argument_role,\n",
    "                        \"mentions\": [\n",
    "                            {\n",
    "                                \"doc_id\": doc_id,\n",
    "                                \"mention\": argument_word,\n",
    "                                # \"span\": {'start': argument_span[0], 'end': argument_span[1]}\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                else:\n",
    "                    nodes_dict[argument_id]['mentions'].append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"mention\": argument_word,\n",
    "                            # \"span\": {'start': argument_span[0], 'end': argument_span[1]}\n",
    "                        }\n",
    "                    )\n",
    "            argument_ids = [argument['argument_id'] for argument in arguments]\n",
    "            sorted_argument_ids = sorted(argument_ids)\n",
    "            # create hyperedge \n",
    "            trigger_id = sub_event['trigger'] \n",
    "            sub_event_hyper_edge_id = trigger_id  + \"-\" + str(doc_id)\n",
    "            sub_event_hyperedges_dict[event_hyperedge_id].append({\n",
    "                'id': sub_event_hyper_edge_id,\n",
    "                'type': \"subevent_hyper_edge\",\n",
    "                \"trigger\": trigger_id,\n",
    "                # \"trigger_type\": trigger_type,\n",
    "                \"arguments\": sorted_argument_ids,\n",
    "                \"doc_id\": doc_id,\n",
    "            })\n",
    "            for argument_id in argument_ids:\n",
    "                sub_event_links[event_hyperedge_id].append((sub_event_hyper_edge_id, argument_id))\n",
    "            event_arguments.update(argument_ids)\n",
    "\n",
    "        event_hyperedges_dict[event_hyperedge_id] = {\n",
    "            'id': event_hyperedge_id,\n",
    "            'type': \"hyper_edge\",\n",
    "            \"trigger\": event_hyperedge_id,\n",
    "            \"arguments\": list(event_arguments),\n",
    "            \"doc_id\": doc_id,\n",
    "            \"summary\": doc['summary'],\n",
    "            \"content\": doc['content']\n",
    "        }\n",
    "        for argument_id in list(event_arguments):\n",
    "            links.append((event_hyperedge_id, argument_id))\n",
    "\n",
    "    return nodes_dict, event_hyperedges_dict, links, sub_event_hyperedges_dict, sub_event_links\n",
    "\n",
    "def merge_RAMS(dataset):\n",
    "    nodes_dict, hyper_edges_dict, links, sub_event_hyperedges, sub_event_links_dict = construct_network(dataset)\n",
    "    B = nx.Graph()\n",
    "    B.add_nodes_from(list(hyper_edges_dict.keys()), bipartite=0)\n",
    "    B.add_nodes_from(list(nodes_dict.keys()), bipartite=1)\n",
    "    B.add_edges_from(links)\n",
    "\n",
    "    return B, nodes_dict, hyper_edges_dict, links, sub_event_hyperedges, sub_event_links_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = json.load(open('data/result/RAMS/gpt_events_dev_linked.json'))\n",
    "B, nodes_dict, hyper_edges_dict, links, sub_event_hyperedges, sub_event_links_dict = merge_RAMS(transformed_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3024, 617]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = hnx.Hypergraph.from_bipartite(B)\n",
    "list(H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_hgraph_data = nx.node_link_data(B)\n",
    "save_json(event_hgraph_data, r'data/result/RAMS/gpt_biHgraph_dev/hgraph.json')\n",
    "save_json(nodes_dict, r'data/result/RAMS/gpt_biHgraph_dev/nodes.json')\n",
    "save_json(hyper_edges_dict, r'data/result/RAMS/gpt_biHgraph_dev/hyperedges.json')\n",
    "\n",
    "save_json(sub_event_hyperedges, r'data/result/RAMS/gpt_biHgraph_dev/sub_event_hyperedges.json')\n",
    "save_json(sub_event_links_dict, r'data/result/RAMS/gpt_biHgraph_dev/sub_event_links.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_frontend(nodes, links, nodes_dict, hyper_edges_dict):\n",
    "    res_nodes = []\n",
    "    res_links = []\n",
    "    for node in nodes:\n",
    "        if node in nodes_dict:\n",
    "            res_nodes.append(nodes_dict[node])\n",
    "        else:\n",
    "            res_nodes.append(hyper_edges_dict[node])\n",
    "    for link in links:\n",
    "        source = link[0]\n",
    "        target = link[1]\n",
    "        res_links.append({\n",
    "            \"source\": source,\n",
    "            \"target\": target,\n",
    "        })\n",
    "    print(len(res_nodes))\n",
    "    return {\n",
    "        \"nodes\": res_nodes, \n",
    "        \"links\": res_links\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3641\n"
     ]
    }
   ],
   "source": [
    "BH = H.bipartite()\n",
    "network = transform_frontend(list(BH.nodes), list(BH.edges), nodes_dict, hyper_edges_dict)\n",
    "save_json(network, 'data/result/RAMS/gpt_dev_hgraph.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_hgraph_preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
