{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "import json\n",
    "from pybrat.parser import BratParser, Entity, Event, Example, Relation\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da80d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_brat_data(data_path):\n",
    "    brat = BratParser(error=\"ignore\")\n",
    "    brat_data = brat.parse(data_path)\n",
    "    return brat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brat_data_to_network(data_path):\n",
    "    G = nx.Graph()\n",
    "    nodes_dict = {} \n",
    "    links = defaultdict(lambda: defaultdict(int))\n",
    "    doc_count = 0\n",
    "    argument_num_dict = defaultdict(int)\n",
    "    for doc in brat_data:\n",
    "        doc_count += 1\n",
    "        origin_id_to_new_id_dict = {}\n",
    "        entity_umls = json.load(open(data_path + doc.id + r'.json'))\n",
    "#         print(doc.id)\n",
    "        for entity in doc.entities:\n",
    "            # TODO: add candidate selection. For now just choosing the first one.\n",
    "            entity_ui = entity.id # reassign node id to either doc_id + original id or CUI\n",
    "            if len(entity_umls[entity.id]) != 0: \n",
    "                entity_ui = entity_umls[entity.id][0]['ui']\n",
    "            else:\n",
    "                # TODO: consider partial matching to disambiguiate entities without CUIs\n",
    "                entity_ui = doc.id + \"-\" + entity_ui\n",
    "            origin_id_to_new_id_dict[entity.id] = entity_ui\n",
    "\n",
    "            if entity_ui not in nodes_dict.keys():\n",
    "                nodes_dict[entity_ui] = {\n",
    "                    \"id\": entity_ui,\n",
    "                    \"type\": \"entity\",\n",
    "                    \"mentions\": [\n",
    "                        {\n",
    "                            \"doc_id\": doc.id, \n",
    "                            \"mention\": entity.mention, \n",
    "                            \"span\": {'start': entity.spans[0].start, 'end': entity.spans[0].end}\n",
    "#                             \"span\": entity.spans\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            else:\n",
    "                nodes_dict[entity_ui][\"mentions\"].append(\n",
    "                    {\n",
    "                        \"doc_id\": doc.id, \n",
    "                        \"mention\": entity.mention, \n",
    "                        \"span\": {'start': entity.spans[0].start, 'end': entity.spans[0].end}\n",
    "\n",
    "\n",
    "#                         \"span\": [entity.spans.start, entity.spans.end]\n",
    "#                         \"span\": entity.spans\n",
    "                    }\n",
    "                )\n",
    "        # each event is treated as a hyper-edge. \n",
    "        # First create a hyper edge node, then connect the hyper edge node with the arguments\n",
    "        # The node id needs to be independent, but the node type is trigger id \n",
    "        for event in doc.events:\n",
    "            trigger_id = origin_id_to_new_id_dict[event.trigger.id]\n",
    "            origin_id_to_new_id_dict[event.id] = trigger_id # account for nested events\n",
    "            argument_ids = list(map(lambda argument: origin_id_to_new_id_dict[argument.id], event.arguments))\n",
    "            \n",
    "            sorted_argument_ids = sorted(argument_ids)\n",
    "            # create hyper edge node\n",
    "            hyper_edge_node_id = trigger_id + \"-\" + \"-\".join(sorted_argument_ids)\n",
    "\n",
    "            if hyper_edge_node_id not in nodes_dict.keys():\n",
    "                nodes_dict[hyper_edge_node_id] = {\n",
    "                    \"id\": hyper_edge_node_id,\n",
    "                    \"type\": \"hyper_edge\",\n",
    "                    \"trigger\": trigger_id,\n",
    "                    \"arguments\": sorted_argument_ids,\n",
    "                    \"mentions\": [\n",
    "                        {\n",
    "                            \"doc_id\": doc.id, \n",
    "                            # TODO: add sentence span\n",
    "                            # \"mention\": entity.mention, \n",
    "                            # \"span\": [entity.spans]\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            else:\n",
    "                nodes_dict[hyper_edge_node_id][\"mentions\"].append(\n",
    "                    {\n",
    "                        \"doc_id\": doc.id,\n",
    "                        # TODO: add sentence span\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # add links between hyper edge node and arguments\n",
    "            for argument_id in argument_ids:\n",
    "                links[hyper_edge_node_id][argument_id] = 1\n",
    "\n",
    "            # argument_num_dict[len(argument_ids)].append(trigger_id)\n",
    "            argument_num_dict[len(argument_ids)] += 1\n",
    "#                 links.append((trigger_id, argument_id, {'attr': 'someAttr'}))\n",
    "#         if doc_count == 10: break\n",
    "\n",
    "    pprint(argument_num_dict)\n",
    "    # turn overlapping links into link length\n",
    "    links_as_list = []\n",
    "    for hyper_edge_node_id, argument_ids in links.items():\n",
    "        for argument_id in argument_ids.keys():\n",
    "            links_as_list.append((hyper_edge_node_id, argument_id))\n",
    "    \n",
    "    # remove nodes that do not have links\n",
    "    G.add_nodes_from([(node_id, node_attribute_dict) for node_id, node_attribute_dict in nodes_dict.items()])\n",
    "    G.add_edges_from(links_as_list)\n",
    "    \n",
    "    G.remove_nodes_from(list(n for n in G.nodes() if G.degree(n) == 0))\n",
    "\n",
    "    print(G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (r'brat-1.3p1/data/all-brat/')\n",
    "brat_data = read_brat_data(data_path)\n",
    "event_network = brat_data_to_network(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_network(G):\n",
    "    # print(G.nodes().data())\n",
    "    colors = list(map(lambda node: 'blue' if node[1]['type'] == 'entity' else 'black', G.nodes().data()))\n",
    "    node_sizes = list(map(lambda node: 100+G.degree(node), G.nodes()))\n",
    "    options = {\n",
    "        \"node_color\": colors,\n",
    "        \"node_size\": node_sizes,\n",
    "        \"width\": 0.5,\n",
    "        \"with_labels\": False,\n",
    "        \"pos\": nx.spring_layout(G, k=0.15)\n",
    "    }\n",
    "    fig = plt.figure(1, figsize=(12, 12), dpi=60)\n",
    "    nx.draw(G, **options)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337409af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_distribution(G, fit_line=True):\n",
    "    degree_sequence = [G.degree(node) for node in G.nodes()]\n",
    "    degree_counts = [(degree, degree_sequence.count(degree)) for degree in set(degree_sequence)]\n",
    "    x, y = zip(*degree_counts)\n",
    "        \n",
    "    # fit line\n",
    "    if fit_line:\n",
    "        filter_degree = 15\n",
    "        filtered_degree_sequence = list(filter(lambda degree: degree < filter_degree, degree_sequence))\n",
    "        filtered_degree_counts = [(degree, degree_sequence.count(degree)) for degree in set(filtered_degree_sequence)]\n",
    "        filtered_x, filtered_y = zip(*filtered_degree_counts)\n",
    "        log_x = np.log10(filtered_x)\n",
    "        log_y = np.log10(filtered_y)\n",
    "        slope, intercept = np.polyfit(log_x, log_y, 1)\n",
    "        print(\"slope:\", slope, \"intercept:\", intercept)\n",
    "        x_vals = np.array([min(filtered_x), max(filtered_x)])\n",
    "        y_vals = 10**(intercept + slope*np.log10(x_vals))\n",
    "        plt.plot(x_vals, y_vals, '--')\n",
    "    \n",
    "        \n",
    "    plt.scatter(x, y)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_highest_degree_node(G, k=0):\n",
    "    # get the degree of each node\n",
    "    degrees = dict(G.degree())\n",
    "    largest_degrees = sorted(degrees, key=degrees.get, reverse=True)\n",
    "    # get the data of the k nodes with the largest degree\n",
    "    data = [G.nodes[node] for node in largest_degrees]\n",
    "#     for k, node in enumerate(data[:50]):\n",
    "#         if node['type'] == 'event':\n",
    "#             print(node['id'], node['type'], degrees[largest_degrees[k]])\n",
    "#         else:\n",
    "#             mentions = list(map(lambda mention_data: mention_data['mention'], node['mentions']))\n",
    "#             print(node['id'], node['type'], degrees[largest_degrees[k]], len(mentions))\n",
    "\n",
    "\n",
    "        \n",
    "    print(f\"The node with the {k} highest degree is {largest_degrees[k]}, with degree {degrees[largest_degrees[k]]}\")\n",
    "#     pprint(data[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_list_bar(G):\n",
    "    # parameters\n",
    "    color_map = {'entity': 'r', 'event': 'b'}\n",
    "    degrees = dict(G.degree())\n",
    "    k = 50\n",
    "    \n",
    "    # prepare bar data\n",
    "    # x\n",
    "    node_list = sorted(degrees, key=degrees.get, reverse=True)[:k]\n",
    "    for node in node_list:\n",
    "        node_data = G.nodes[node]\n",
    "        print(node_data['id'], node_data['type'], G.degree(node))\n",
    "    # y\n",
    "    degree_list = [G.degree(node) for node in node_list]\n",
    "    # color\n",
    "    type_list = [G.nodes[node]['type'] for node in node_list]\n",
    "    color_list = [color_map[type] for type in type_list]\n",
    "    \n",
    "    # plot\n",
    "    fig,a = plt.subplots()\n",
    "\n",
    "    a.bar(node_list, degree_list, color=color_list, edgecolor='white', linewidth=1)\n",
    "    a.xaxis.set_visible(False)\n",
    "\n",
    "    # remove x-axis label\n",
    "#     ax.set(xlabel=None)\n",
    "\n",
    "    \n",
    "    # add a legend for the color map\n",
    "    legend_list = [plt.Rectangle((0,0),1,1,color=color_map[node_type]) for node_type in color_map.keys()]\n",
    "    plt.legend(legend_list, color_map.keys())\n",
    "\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19792136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_community_detection(G, alg='louvain'):\n",
    "    if alg == 'louvain':\n",
    "        return community_louvain.best_partition(G, weight='strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14cf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_layout(g, partition):\n",
    "    \"\"\"\n",
    "    Compute the layout for a modular graph.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    g -- networkx.Graph or networkx.DiGraph instance\n",
    "        graph to plot\n",
    "\n",
    "    partition -- dict mapping int node -> int community\n",
    "        graph partitions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos -- dict mapping int node -> (float x, float y)\n",
    "        node positions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pos_communities = _position_communities(g, partition, scale=3.)\n",
    "\n",
    "    pos_nodes = _position_nodes(g, partition, scale=1.)\n",
    "\n",
    "    # combine positions\n",
    "    pos = dict()\n",
    "    for node in g.nodes():\n",
    "        pos[node] = pos_communities[node] + pos_nodes[node]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _position_communities(g, partition, **kwargs):\n",
    "\n",
    "    # create a weighted graph, in which each node corresponds to a community,\n",
    "    # and each edge weight to the number of edges between communities\n",
    "    between_community_edges = _find_between_community_edges(g, partition)\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    hypergraph = nx.DiGraph()\n",
    "    hypergraph.add_nodes_from(communities)\n",
    "    for (ci, cj), edges in between_community_edges.items():\n",
    "        hypergraph.add_edge(ci, cj, weight=len(edges))\n",
    "\n",
    "    # find layout for communities\n",
    "    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n",
    "\n",
    "    # set node positions to position of community\n",
    "    pos = dict()\n",
    "    for node, community in partition.items():\n",
    "        pos[node] = pos_communities[community]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _find_between_community_edges(g, partition):\n",
    "\n",
    "    edges = dict()\n",
    "\n",
    "    for (ni, nj) in g.edges():\n",
    "        ci = partition[ni]\n",
    "        cj = partition[nj]\n",
    "\n",
    "        if ci != cj:\n",
    "            try:\n",
    "                edges[(ci, cj)] += [(ni, nj)]\n",
    "            except KeyError:\n",
    "                edges[(ci, cj)] = [(ni, nj)]\n",
    "\n",
    "    return edges\n",
    "\n",
    "def _position_nodes(g, partition, **kwargs):\n",
    "    \"\"\"\n",
    "    Positions nodes within communities.\n",
    "    \"\"\"\n",
    "\n",
    "    communities = dict()\n",
    "    for node, community in partition.items():\n",
    "        try:\n",
    "            communities[community] += [node]\n",
    "        except KeyError:\n",
    "            communities[community] = [node]\n",
    "\n",
    "    pos = dict()\n",
    "    for ci, nodes in communities.items():\n",
    "        subgraph = g.subgraph(nodes)\n",
    "        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n",
    "        pos.update(pos_subgraph)\n",
    "\n",
    "    return pos\n",
    "\n",
    "def visualize_community(G, partition):\n",
    "    # to install networkx 2.0 compatible version of python-louvain use:\n",
    "    # pip install -U git+https://github.com/taynaud/python-louvain.git@networkx2\n",
    "    from community import community_louvain\n",
    "\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    pos = community_layout(G, partition)\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12), dpi=60)\n",
    "#     pprint(partition)\n",
    "    nx.draw(G, pos, node_color=list(partition.values()), node_size=40)\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (r'brat-1.3p1/data/all-brat/')\n",
    "brat_data = read_brat_data(data_path)\n",
    "event_network = brat_data_to_network(data_path)\n",
    "print(event_network.number_of_nodes(), event_network.number_of_edges())\n",
    "# pprint(communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out events that only occur once\n",
    "# filtered_nodes = [n for n in event_network.nodes() if event_network.nodes[n]['type']=='entity']\n",
    "# filtered_nodes = [event_network.nodes[n]['type'] for n in event_network.nodes()]\n",
    "\n",
    "\n",
    "filtered_nodes = [n for n in event_network.nodes() if event_network.nodes[n]['type']=='hyper_edge' and event_network.degree(n) <= 1]\n",
    "# pprint(filtered_nodes)\n",
    "# filtered_nodes = [n for n in event_network.nodes() if event_network.degree(n) <= 1]\n",
    "event_network.remove_nodes_from(filtered_nodes)\n",
    "event_network.remove_nodes_from([n for n in event_network.nodes() if event_network.degree(n) == 0])\n",
    "\n",
    "print(event_network.number_of_nodes(), event_network.number_of_edges())\n",
    "\n",
    "\n",
    "# draw_network(event_network)\n",
    "# plot_degree_distribution(event_network)\n",
    "# plot_degree_list_bar(event_network)\n",
    "# communities = run_community_detection(event_network)\n",
    "\n",
    "# visualize_community(event_network, communities)\n",
    "# get_k_highest_degree_node(event_network, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e91863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "event_network_data = nx.node_link_data(event_network)\n",
    "pprint(event_network_data)\n",
    "save_json(event_network_data, r'event_network_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hyper_edges(G, filepath=r'hyper_edges.txt'):\n",
    "    hyper_edges = [n for n in event_network.nodes() if event_network.nodes[n]['type']=='hyper_edge']\n",
    "    entities = [n for n in event_network.nodes() if event_network.nodes[n]['type']=='entity']\n",
    "    # assuming no event-event connection\n",
    "    node_to_index = {node: i+1 for i, node in enumerate(entities)}\n",
    "    \n",
    "    save_json(node_to_index, 'node_to_index.json')\n",
    "    with open('hyper_edges.txt', 'w', encoding='utf-8') as f:\n",
    "        for hyper_edge in hyper_edges:\n",
    "            # assuming no event-event connection\n",
    "            edge_nodes = [str(node_to_index[n]) for n in G.neighbors(hyper_edge) if event_network.nodes[n]['type']=='entity']\n",
    "#             edge_nodes = [n for n in G[hyper_edge]]\n",
    "            \n",
    "#             if len(edge_nodes) == 1:\n",
    "#                 pprint(hyper_edge)\n",
    "#                 pprint([event_network.nodes[n] for n in edge_nodes])\n",
    "\n",
    "            if len(edge_nodes) > 2:\n",
    "                print(len(edge_nodes))\n",
    "            line = ','.join(edge_nodes)\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "    f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91caa9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hyper_edges(event_network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
