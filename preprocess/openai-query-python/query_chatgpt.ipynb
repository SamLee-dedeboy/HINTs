{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import jsonlines\n",
    "from flask import Flask, redirect, render_template, request, url_for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_chatgpt(prompt):\n",
    "    # input_filepath = r'./data/tweets/{}/'.format(case_name)\n",
    "    original_url = \"http://127.0.0.1:5000/event_extraction\"\n",
    "    body = {\"prompt\": prompt}\n",
    "    response = requests.post(original_url, json=body).json()\n",
    "    gpt_response = response['choices'][0]['text'].strip()\n",
    "    return gpt_response\n",
    "    \n",
    "def request_chatgpt_gpt4(messages):\n",
    "    # input_filepath = r'./data/tweets/{}/'.format(case_name)\n",
    "    original_url = \"http://127.0.0.1:5000/event_extraction\"\n",
    "    body = {\"messages\": messages}\n",
    "    response = requests.post(original_url, json=body).json()\n",
    "    gpt_response = response['choices'][0]['message']['content'].strip()\n",
    "    return gpt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_reader = jsonlines.open(r'../data/raw/RAMS/dev.jsonlines')\n",
    "dataset = [datum for datum in dev_reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(datum_sentences):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in datum_sentences] # merge the words into sentences\n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = merge_sentences(dataset[4]['sentences'])\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments(article):\n",
    "    prompt = \"\"\"\n",
    "    Below is a news article of an event.\n",
    "    Please describe the main characters that the news article discussed, the character can be any organization, person or location.\n",
    "    It can have one or more characters.\n",
    "    Reply in the format '[character 1] [character 2]...'\n",
    "    Article: \\n {article}\n",
    "    \"\"\".format(article=article)\n",
    "    arguments = request_chatgpt(prompt)\n",
    "    return arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_sentence(article, arguments):\n",
    "    prompt = \"\"\"\n",
    "    Below is a news article of an event.\n",
    "    The major participants in the articles are: {participants}.\n",
    "    Please describe what the article discussed about them in one sentence.\n",
    "    Reply starts with 'The article discussed ...'\n",
    "    Article: \\n {article}\n",
    "    \"\"\".format(participants=\", \".join(arguments), article=article)\n",
    "    sentence = request_chatgpt(prompt)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dataset = []\n",
    "for datum in dataset:\n",
    "    saved_datum = {}\n",
    "    article = merge_sentences(datum['sentences'])\n",
    "    arguments = get_arguments(article)\n",
    "    sentence = summarize_sentence(article, arguments)\n",
    "    print(sentence)\n",
    "    saved_datum['content'] = datum['sentences']\n",
    "    saved_datum['url'] = datum['source_url']\n",
    "    saved_datum['summary'] = sentence\n",
    "    saved_dataset.append(saved_datum)\n",
    "save_json(saved_dataset, r'../data/raw/RAMS/summarized/dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_sentence(sentence):\n",
    "    if sentence.startswith('The article discussed how'):\n",
    "        stripped_sentence = sentence.replace('The article discussed how', '').strip()\n",
    "    elif sentence.startswith('The article discussed'):\n",
    "        stripped_sentence = sentence.replace('The article discussed', '').strip()\n",
    "    else:\n",
    "        print(\"!!!\")\n",
    "    return stripped_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events(sentence):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are an event extraction system. Please extract the events from user provided sentence.\n",
    "                An 'event' should contain one or more 'participants', which are the major participants in the event,\n",
    "                and a 'trigger', which is a verb that describes what happens between the participants.\n",
    "                The triggers and participants should be human-readable.\n",
    "                Reply with each line being an event in the format:\n",
    "                [trigger1], [participant 1], [participant 2], ...\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"Trump's inability to work with people beyond his base, as demonstrated by his comparison to Saddam Hussein's Iraq, is a major problem for the United States, as it requires the president to build bridges and form alliances in order to get things done.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"Problem, Trump, United States; \\n Inable, Trump, work with, people beyond his base; \\n Compare, Trump, Saddam Hussein's Iraq; \\n Require, president, build bridges and form alliances;\"},\n",
    "        { \"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "    # \"\"\"\n",
    "    # An event graph describes an event in graph structure.\n",
    "    # It should contain one or more 'participants', which are the major participants in the event,\n",
    "    # and a 'trigger', which is a verb that describes what happens between the participants.\n",
    "    # Reorganize the sentence below into one or more event graph.\n",
    "    # The triggers and participants should be human-readable.\n",
    "    # Reply with each line being an event graph in the format:\n",
    "    # [trigger1], [participant 1], [participant 2], ...\n",
    "    # [trigger2], [participant 1], [participant 2], ...\n",
    "    # Sentence: {sentence}\n",
    "    # \"\"\".format(sentence=sentence)\n",
    "    events = request_chatgpt_gpt4(messages)\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAMS_summarized = json.load(open(r'../data/raw/RAMS/summarized/dev.json'))\n",
    "res_events = []\n",
    "error_datum = []\n",
    "for index, datum in enumerate(RAMS_summarized):\n",
    "    try:\n",
    "        print('{}/{}'.format(index, len(RAMS_summarized)))\n",
    "        sentence = strip_sentence(datum['summary'])\n",
    "        events = extract_events(sentence)\n",
    "        datum['events'] = events\n",
    "        res_events.append(datum)\n",
    "    except:\n",
    "        error_datum.append(datum)\n",
    "save_json(res_events, r'../data/raw/RAMS/events/dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def post_process_events(dataset):\n",
    "    for index, datum in enumerate(dataset):\n",
    "        datum['doc_id'] = index\n",
    "        datum['events_raw'] = datum['events']\n",
    "        events_str = datum['events'].split('\\n')\n",
    "        events = []\n",
    "        for event_str in events_str:\n",
    "            components = event_str.split(',')\n",
    "            trigger = components[0].strip()\n",
    "            arguments = [arg.strip().strip(punctuation) for arg in components[1:]]\n",
    "            events.append({'trigger': trigger, 'arguments': arguments})\n",
    "        datum['events'] = events\n",
    "    return dataset\n",
    "\n",
    "dataset = json.load(open(r'../data/raw/RAMS/events/dev.json'))\n",
    "processed_dataset = post_process_events(dataset)\n",
    "save_json(processed_dataset, r'../data/result/RAMS/gpt_events_dev.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_hgraph_preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
