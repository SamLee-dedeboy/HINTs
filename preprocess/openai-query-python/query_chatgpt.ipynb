{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import jsonlines\n",
    "from flask import Flask, redirect, render_template, request, url_for\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_chatgpt(prompt):\n",
    "    # input_filepath = r'./data/tweets/{}/'.format(case_name)\n",
    "    original_url = \"http://127.0.0.1:5000/event_extraction\"\n",
    "    body = {\"prompt\": prompt}\n",
    "    response = requests.post(original_url, json=body).json()\n",
    "    gpt_response = response['choices'][0]['text'].strip()\n",
    "    return gpt_response\n",
    "    \n",
    "def request_chatgpt_gpt4(messages, functions=None):\n",
    "    # input_filepath = r'./data/tweets/{}/'.format(case_name)\n",
    "    original_url = \"http://127.0.0.1:5000/event_extraction\"\n",
    "    if functions is None:\n",
    "        body = {\"messages\": messages}\n",
    "        response = requests.post(original_url, json=body).json()\n",
    "        gpt_response = response['choices'][0]['message']['content'].strip()\n",
    "        return gpt_response\n",
    "    else:\n",
    "        body = {\"messages\": messages, \"functions\": functions}\n",
    "        response = requests.post(original_url, json=body).json()\n",
    "        response_messages = response['choices'][0]['message']\n",
    "        if response_messages.get(\"function_call\"):\n",
    "            function_args = json.loads(response_messages['function_call']['arguments'])\n",
    "            print(\"function called!\")\n",
    "            return function_args\n",
    "        else:\n",
    "            return response_messages['content'].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAMS\n",
    "dev_reader = jsonlines.open(r'../data/raw/RAMS/dev.jsonlines')\n",
    "dataset = [datum for datum in dev_reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All The News 1.0\n",
    "articles1_csv = csv.DictReader(open(\"../data/raw/AllTheNews/articles1.csv\"))\n",
    "articles2_csv = csv.DictReader(open(\"../data/raw/AllTheNews/articles2.csv\"))\n",
    "articles3_csv = csv.DictReader(open(\"../data/raw/AllTheNews/articles3.csv\"))\n",
    "dataset = [datum for datum in articles1_csv] + [datum for datum in articles2_csv] + [datum for datum in articles3_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean AllTheNews\n",
    "dataset_2016 = [datum for datum in dataset if datum['year'] == '2016.0']\n",
    "articles_grouped = defaultdict(list)\n",
    "for datum in dataset_2016:\n",
    "    articles_grouped[datum['publication']].append(datum)\n",
    "random_selected_dataset = []\n",
    "for publication, articles in articles_grouped.items():\n",
    "    total_articles = len(articles)\n",
    "    random_samples = random.sample(articles, math.floor(total_articles/10))\n",
    "    print(len(random_samples), total_articles)\n",
    "    random_selected_dataset += random_samples\n",
    "save_json(random_selected_dataset, r'../data/raw/AllTheNews/cleaned/2016_10p.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(datum_sentences):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in datum_sentences] # merge the words into sentences\n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments(article, model=\"gpt-3.5-turbo-0613\"):\n",
    "    # if model == \"gpt-3.5-turbo-0613\":\n",
    "    #     functions = [\n",
    "    #         {\n",
    "    #             \"name\": \"get_characters\",\n",
    "    #             \"description\": \"Get the main characters of the news article, which can be any organization, person or location\",\n",
    "    #             \"parameters\": {\n",
    "    #                 \"type\": \"object\",\n",
    "    #                 \"properties\": {\n",
    "    #                     \"characters\": { \"type\": \"array\", \"items\": { \"type\": \"string\" }}\n",
    "    #                 },\n",
    "    #             },\n",
    "    #         }\n",
    "    #     ]\n",
    "    #     messages = [ \n",
    "    #         {\n",
    "    #             \"role\": \"system\", \n",
    "    #             \"content\": \"\"\"\n",
    "    #                 You are an extraction system that extracts the main characters of a news article.\n",
    "    #                 The main characters can be any organization, person or location.\n",
    "    #                 The user will provide you with a news article to extract.\n",
    "    #             \"\"\"\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"role\": \"user\", \"content\": article\n",
    "    #         } \n",
    "    #     ]\n",
    "    #     arguments = request_chatgpt_gpt4(messages, functions)\n",
    "    if model == \"gpt-3.5-turbo-0613\":\n",
    "        messages = [ \n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"\"\"\n",
    "                    You are an extraction system that extracts the main characters of a news article.\n",
    "                    The main characters can be any organization, person or location that heavily involved in the event described by the news article.\n",
    "                    The user will provide you with a news article to extract.\n",
    "                    Reply in the format '[character 1] [character 2]...'\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \"content\": article\n",
    "            } \n",
    "        ]\n",
    "        arguments = request_chatgpt_gpt4(messages)\n",
    "\n",
    "    else:\n",
    "        prompt = \"\"\"\n",
    "        Below is a news article of an event.\n",
    "        Please describe the main characters that the news article discussed, the character can be any organization, person or location.\n",
    "        It can have one or more characters.\n",
    "        Reply in the format '[character 1] [character 2]...'\n",
    "        Article: \\n {article}\n",
    "        \"\"\".format(article=article)\n",
    "        arguments = request_chatgpt(prompt)\n",
    "    return arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_sentence(article, arguments, model=\"gpt-3.5-turbo-0613\"):\n",
    "    if model == \"gpt-3.5-turbo-0613\":\n",
    "        messages = [ \n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"\"\"\n",
    "                    You are an summarization system that summarizes the events that happened between the main characters of a news article.\n",
    "                    The user will provide you with a list of main characters and a news article to summarize.\n",
    "                    Try to summarize the article with no more than three sentences. \n",
    "                    Reply starts with 'The article discussed ...'\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \"content\": \"Main Characters:\\n{arguments} \\n\\n\\n Article: {article}\".format(arguments=arguments, article=article)\n",
    "            } \n",
    "        ]\n",
    "        sentence = request_chatgpt_gpt4(messages)\n",
    "    else:\n",
    "        prompt = \"\"\"\n",
    "        Below is a news article of an event.\n",
    "        The major participants in the articles are: {participants}.\n",
    "        Please describe what the article discud about them in one sentence.\n",
    "        Reply starts with 'The article discussed ...'\n",
    "        Article: \\n {article}\n",
    "        \"\"\".format(participants=\", \".join(arguments), article=article)\n",
    "        sentence = request_chatgpt(prompt)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8534\n",
      "2/8534\n",
      "3/8534\n",
      "4/8534\n",
      "5/8534\n"
     ]
    }
   ],
   "source": [
    "# All The News\n",
    "dataset = json.load(open(r'../data/raw/AllTheNews/cleaned/2016_10p.json'))\n",
    "saved_dataset = []\n",
    "count = 0\n",
    "for datum in dataset:\n",
    "    try:\n",
    "        count += 1\n",
    "        if count == 6: break\n",
    "        print(\"{}/{}\".format(count, len(dataset)))\n",
    "        saved_datum = {}\n",
    "        article = datum['content']\n",
    "        arguments = get_arguments(article)\n",
    "        sentence = summarize_sentence(article, arguments)\n",
    "        saved_datum['id'] = datum['id']\n",
    "        saved_datum['content'] = datum['content']\n",
    "        saved_datum['title'] = datum['title']\n",
    "        saved_datum['publication'] = datum['publication']\n",
    "        saved_datum['author'] = datum['author']\n",
    "        saved_datum['url'] = datum['url']\n",
    "        saved_datum['date'] = datum['date']\n",
    "        saved_datum['summary'] = sentence\n",
    "        saved_dataset.append(saved_datum)\n",
    "    except:\n",
    "        continue\n",
    "save_json(saved_dataset, r'../data/raw/AllTheNews/summarized/2016_10p.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAMS\n",
    "saved_dataset = []\n",
    "for datum in dataset:\n",
    "    saved_datum = {}\n",
    "    article = merge_sentences(datum['sentences'])\n",
    "    arguments = get_arguments(article)\n",
    "    sentence = summarize_sentence(article, arguments)\n",
    "    print(sentence)\n",
    "    saved_datum['content'] = datum['sentences']\n",
    "    saved_datum['url'] = datum['source_url']\n",
    "    saved_datum['summary'] = sentence\n",
    "    saved_dataset.append(saved_datum)\n",
    "save_json(saved_dataset, r'../data/raw/RAMS/summarized/dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_sentence(sentence):\n",
    "    if sentence.startswith('The article discussed how'):\n",
    "        stripped_sentence = sentence.replace('The article discussed how', '').strip()\n",
    "    elif sentence.startswith('The article discussed'):\n",
    "        stripped_sentence = sentence.replace('The article discussed', '').strip()\n",
    "    else:\n",
    "        print(\"!!!\")\n",
    "    return stripped_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events(sentence):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are an event extraction system. Please extract the events from user provided sentence.\n",
    "                An 'event' should contain one or more 'participants', which are the major participants in the event,\n",
    "                and a 'trigger', which is a verb that describes what happens between the participants.\n",
    "                The triggers and participants should be human-readable.\n",
    "                Reply with each line being an event in the format:\n",
    "                [trigger1], [participant 1], [participant 2], ...\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"Trump's inability to work with people beyond his base, as demonstrated by his comparison to Saddam Hussein's Iraq, is a major problem for the United States, as it requires the president to build bridges and form alliances in order to get things done.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"Problem, Trump, United States; \\n Inable, Trump, work with, people beyond his base; \\n Compare, Trump, Saddam Hussein's Iraq; \\n Require, president, build bridges and form alliances;\"},\n",
    "        { \"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "    events = request_chatgpt_gpt4(messages)\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5\n",
      "1/5\n",
      "2/5\n",
      "3/5\n",
      "4/5\n"
     ]
    }
   ],
   "source": [
    "# AllTheNews\n",
    "AllTheNews_summarized = json.load(open(r'../data/raw/AllTheNews/summarized/2016_10p.json'))\n",
    "res_events = []\n",
    "error_datum = []\n",
    "for index, datum in enumerate(AllTheNews_summarized):\n",
    "    print('{}/{}'.format(index, len(AllTheNews_summarized)))\n",
    "    sentence = strip_sentence(datum['summary'])\n",
    "    events = extract_events(sentence)\n",
    "    datum['events'] = events\n",
    "    res_events.append(datum)\n",
    "save_json(res_events, r'../data/raw/AllTheNews/events/2016_10p.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAMS_summarized = json.load(open(r'../data/raw/RAMS/summarized/dev.json'))\n",
    "res_events = []\n",
    "error_datum = []\n",
    "for index, datum in enumerate(RAMS_summarized):\n",
    "    try:\n",
    "        print('{}/{}'.format(index, len(RAMS_summarized)))\n",
    "        sentence = strip_sentence(datum['summary'])\n",
    "        events = extract_events(sentence)\n",
    "        datum['events'] = events\n",
    "        res_events.append(datum)\n",
    "    except:\n",
    "        error_datum.append(datum)\n",
    "save_json(res_events, r'../data/raw/RAMS/events/dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def post_process_events(dataset):\n",
    "    for index, datum in enumerate(dataset):\n",
    "        datum['doc_id'] = index\n",
    "        datum['events_raw'] = datum['events']\n",
    "        events_str = datum['events'].split('\\n')\n",
    "        events = []\n",
    "        for event_str in events_str:\n",
    "            components = event_str.split(',')\n",
    "            trigger = components[0].strip()\n",
    "            arguments = [arg.strip().strip(punctuation) for arg in components[1:]]\n",
    "            events.append({'trigger': trigger, 'arguments': arguments})\n",
    "        datum['events'] = events\n",
    "    return dataset\n",
    "\n",
    "dataset = json.load(open(r'../data/raw/RAMS/events/dev.json'))\n",
    "processed_dataset = post_process_events(dataset)\n",
    "save_json(processed_dataset, r'../data/result/RAMS/gpt_events_dev.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_hgraph_preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
