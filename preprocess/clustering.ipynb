{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on the event hyper graph\n",
    "### Node similarity measure:\n",
    "- topological overlap + event/sentence embeddings\n",
    "### Group similarity measure:\n",
    "- average similarity of node pairs between cluster pairs\n",
    "### Procedure:\n",
    "- convert hypergraph to weighted normal graph (or don't?)\n",
    "- Assign each node to its own cluster and evaluate similarity measure for all node pairs\n",
    "- merge node pairs with highest similarity measure into the same community \n",
    "- - how many pairs to merge?\n",
    "- repeat by merging clusters in the same way until no merge is available\n",
    "### dual of hypergraph\n",
    "- link clustering can be achieved by doing clustering on the dual of a hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'graph_tool', 'infomap', 'wurlitzer', 'leidenalg', 'karateclub'}\n",
      "Note: to be able to use all overlapping methods, you need to install some additional packages:  {'karateclub', 'ASLPAw'}\n",
      "Note: to be able to use all bipartite methods, you need to install some additional packages:  {'infomap', 'wurlitzer', 'leidenalg'}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import hypernetx as hnx\n",
    "import numpy as np\n",
    "import json\n",
    "import hypernetx.algorithms.hypergraph_modularity as hmod\n",
    "from cdlib.algorithms import ilouvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(H, u, v):\n",
    "    return len(list(hnx.common_neighbors(H, u, v)))\n",
    "\n",
    "def node_similarity(node1, node2):\n",
    "    return 1\n",
    "def cluster_similarity(cluster1, cluster2):\n",
    "    \"\"\"\n",
    "    cluster1, cluster2: list of nodes\n",
    "    \"\"\"\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read network\n",
    "B = nx.node_link_graph(json.load(open('data/result/RAMS/biHgraph_dev/hgraph.json')))\n",
    "H = hnx.Hypergraph.from_bipartite(B)\n",
    "list(H.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce hypergraph to two-section graph with edge reweighting proposed in [1]\n",
    "\n",
    "\n",
    "[1] Kumar T., Vaidyanathan S., Ananthapadmanabhan H., Parthasarathy S. and Ravindran B. “A New Measure of Modularity in Hypergraphs: Theoretical Insights and Implications for Effective Clustering”. In: Cherifi H., Gaito S., Mendes J., Moro E., Rocha L. (eds) Complex Networks and Their Applications VIII. COMPLEX NETWORKS 2019. Studies in Computational Intelligence, vol 881. Springer, Cham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_kumar(HG, node_embeddings, delta=0.01):\n",
    "    \"\"\"\n",
    "    Compute a partition of the vertices in hypergraph HG as per Kumar's algorithm [1]_\n",
    "    But instead of using normal clustering, use the modified clustering algorithm that considers embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    HG : Hypergraph\n",
    "\n",
    "    node_embeddings : dict of node -> { node: dict of embeddings -> { 0: d0, 1: d1, 2: d2, ...} }\n",
    "\n",
    "    delta : float, optional\n",
    "        convergence stopping criterion\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    : list of sets\n",
    "       A partition of the vertices in HG\n",
    "\n",
    "    \"\"\"\n",
    "    # weights will be modified -- store initial weights\n",
    "    W = {\n",
    "        e: HG.edges[e].weight for e in HG.edges\n",
    "    }  # uses edge id for reference instead of int\n",
    "    # build graph\n",
    "    G = hmod.two_section(HG)\n",
    "    # apply clustering\n",
    "    # TODO: use modified clustering algorithm\n",
    "    # CG = G.community_multilevel(weights=\"weight\")\n",
    "    CG = ilouvain(G, node_embeddings, id)\n",
    "    CH = []\n",
    "    for comm in CG.as_cover():\n",
    "        CH.append(set([G.vs[x][\"name\"] for x in comm]))\n",
    "    # LOOP\n",
    "    diff = 1\n",
    "    ctr = 0\n",
    "    while diff > delta:\n",
    "        # re-weight\n",
    "        diff = 0\n",
    "        for e in HG.edges:\n",
    "            edge = HG.edges[e]\n",
    "            reweight = (\n",
    "                sum([1 / (1 + HG.size(e, c)) for c in CH])\n",
    "                * (HG.size(e) + len(CH))\n",
    "                / HG.number_of_edges()\n",
    "            )\n",
    "            diff = max(diff, 0.5 * abs(edge.weight - reweight))\n",
    "            edge.weight = 0.5 * edge.weight + 0.5 * reweight\n",
    "        # re-run louvain\n",
    "        # build graph\n",
    "        G = hmod.two_section(HG)\n",
    "        # apply clustering\n",
    "        # TODO: use modified clustering algorithm\n",
    "        CG = G.community_multilevel(weights=\"weight\")\n",
    "        CH = []\n",
    "        for comm in CG.as_cover():\n",
    "            CH.append(set([G.vs[x][\"name\"] for x in comm]))\n",
    "        ctr += 1\n",
    "        if ctr > 50:  # this process sometimes gets stuck -- set limit\n",
    "            break\n",
    "    G.vs[\"part\"] = CG.membership\n",
    "    for e in HG.edges:\n",
    "        HG.edges[e].weight = W[e]\n",
    "    return hmod.dict2part({v[\"name\"]: v[\"part\"] for v in G.vs})    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = embedding_kumar(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(partitions)\n",
    "for partition in partitions:\n",
    "    print(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use I-louvain implemented in CDLib to do attributed node clustering\n",
    "- the attributes are the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_hgraph_preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
