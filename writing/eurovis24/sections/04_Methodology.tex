\section{Methodology}\label{sec: methodology}
As shown in~\autoref{fig:pipeline}, starting from a corpus of unstructured texts,
we first use LLMs to extract the main keywords from each document.
The keywords are disambiguated and linked to a knowledge base if available.
We create and store document embeddings and keyword embeddings for further usage.
All LLM-based preprocessing used OpenAI's `gpt-3.5-turbo-16k-0613' model.
We provide all our prompts in supplemental material.
In the next stage, we construct a document hypergraph and a keyword hypergraph, which are then clustered separately by combining connectivity similarity and semantic similarity.
The clustering result is hosted on the server and visualized in an interactive user interface.
Below, we describe each component in detail.
\subsection{Data Preparation}\label{sec: preprocessing}
The data preparation stage consists of five steps: summarization, keyword extraction, keyword disambiguation, document and keyword embedding, and topic label generation.
We use LLMs to perform all five steps as they outperform traditional approaches in both accuracy and flexibility.
Note that since the user does not directly interact with the LLM in our system, we do not consider our system as an agent-based system, as will be discussed in~\autoref{sec: user_study}.
% \subsubsection{Datasets}
% The scope of this research paper is to visualize unstructured data conveying any type of information. 
% We evaluate our system on two different datasets: the \textit{All The News} dataset~\cite{allthenews} and a visualization publication dataset (VisPub)~\cite{vispub}.
% \begin{itemize}
%     \item \textbf{All The News} \, This dataset contains 2.7 million articles from 27 American publications.  It typically includes metadata such as headlines, publication dates, and the full text of the articles. For our purpose, we only use the full text of the articles in our pipeline. We down-sampled the dataset to contain only 10 percent of articles published in 2016. This resulted in a dataset of 8192 articles. We used this subset to evaluate our system in a case study.
%     \item \textbf{Visualization Publications Dataset (VisPub)} \, This dataset contains 3620 IEEE visualization publications from 1990 to 2022. We removed 79 articles that do not have abstracts available. These research papers cover a wide variety of research topics in visualization, which is not necessarily known to LLMs. We tested our system on this dataset to show that our system does not rely on the parametric knowledge learned by LLMs during training.
% \end{itemize}
\subsubsection{Summarization}\label{sec: summarization}
Unstructured text documents can contain a vast amount of information, among which certain information is more important than others.
Summarization helps condense the content into a concise form, making it easier for users to make sense of and LLMs to process and extract the most important information. 
Zhang et al\cite{zhang2023extractive} reported that human users found the summaries generated by ChatGPT to be more interpretable and trustworthy than traditional approaches, which makes LLMs a better choice for our purpose.
% For the VisPub dataset, the abstracts of research papers already convey the main research idea in a condensed form, therefore the abstracts were used as a summary of the document.
We employed an instruction-based zero-shot prompt, where the model is instructed to act as a text summarizer given the document's content.
% \textit{`You are a summarization system that summarizes the events that happened between the main keywords of a news article.
% The user will provide you with a news article to summarize.
% Try to summarize the article with no more than three sentences. 
% Reply starts with 'The article discussed ...''}.
% Although we instructed the model to generate a summary with no more than three sentences, we found that the model often does not adhere to that instruction.
% It is simply a signal to the model that the summary should be concise.
These summaries are used in subsequent preprocessing stages as well as in the user interface.
The summarization module can be replaced with any other summarization model or more advanced LLM summarization techniques if desired, but in early prototyping, we found that a zero-shot prompt is good enough for most datasets.

\subsubsection{Keyword Extraction}\label{sec: keyword_extraction}
The keyword extraction builds upon the summaries generated in the previous step.
We define a keyword as entities or concepts that are significantly discussed in the documents.
For news articles, a keyword can be a named entity such as a person, place or location.
For research papers, a keyword can be a model, a technique or an algorithm proposed or used by the paper.
\textbf{TODO: Move to related works: 
Previous works use computational metrics such as TF-IDF or \textit{saliency} to extract keywords.
However, computational metrics often cannot well encode high-level semantics.
Instead, we rely on LLMs' ability to understand natural language to identify significant keywords from unstructured texts.
}

We broke down the process of keyword extraction into two steps to avoid the problem of extrinsic hallucination as discussed by Bang et al.~\cite{bang2023multitask}.
First, a sentence describing the most important event is generated from the summary. 
The event should be the primary focus of the news article and should describe the major objective of the news article, such as a geopolitical event like the G20 summit.
Then a few-shot prompt was adopted to give the LLM examples of what kind of events would be described as the main event. 
Next, all the main keywords involved in that event are extracted with another few-shot prompt.
The prompt transforms an extraction task into a question-answering task.
An example prompt for a news article dataset is 
\textit{`A major event is reported by a news article: \{event\_description\}, what are the main keywords that are majorly involved in the event?'.}
Providing the model with more context yields better accuracy and lower chances of generating a hallucinated output.
The extracted keywords will then be disambiguated and used in the \textit{Model} stage.

\subsubsection{Keyword Disambiguation}\label{sec: keyword_disambiguation}
\textbf{TODO: try new LLM disambiguation method: embed explanation, find close ones, use prompt to disambiguate}
Once the main keywords are extracted, a disambiguation step is done to create connections between documents.
In early experiments, we found that LLMs do not excel at disambiguating keywords.
We thus prioritize using an entity linking model to disambiguate keywords.
Entity linking is the task of mapping a named entity text mentions to their corresponding entities in a knowledge base~\cite{shen2014entity}.
Existing models adopt a supervised learning approach, where a large amount of training data is required, so they are usually limited to a specific domain.
For datasets that an existing entity-linking model~\cite{ayoola2022refined} is available, we use it to disambiguate keywords.
% For example, to process the \textit{All The News} dataset, we used ReFinED~\cite{ayoola2022refined}, an entity linking model that maps text mentions to entities in Wikipedia or Wikidata.
In cases where the keywords cannot be linked to an external knowledge base, we employ a matching algorithm based on embeddings.
The algorithm is straightforward:
For every main keyword extracted in the previous step, we embed them and create a pair-wise similarity matrix.
Then the pairs of keywords with a similarity score above a threshold are considered to be the same keyword.
We then use LLMs to generate a unified title for the matched keywords, which is used as the keyword's label in the user interface.

\subsubsection{Document and Keyword Embedding}\label{sec: embeddings} 
Embeddings are dense vectors that can be used to measure similarities between objects.
In text analysis, embeddings are often created on words, sentences or documents.
We adopted a similar approach here for both documents and keywords.
For documents, we embed the summaries generated in~\autoref{sec: summarization}.
For keywords that can be linked to an external knowledge base,
we embed the description section of the keyword in the knowledge base.
% Otherwise, we embed all text mentions that appear in the corpus and average them to generate the embedding for a disambiguated keyword.
Otherwise, we first generate a description for the keyword using LLMs and then embed the description.
The description is generated with a simple prompt: \textit{`What is \{keyword\}?'}.
In cases where domain knowledge is needed to generate a good description, a RAG-based (\textbf{TODO: cite{RAG}}) prompt can be used.
We used OpenAI's `text-embedding-ada-002' model for all embeddings. 
This allows us to measure the semantic similarity between documents, keywords and user queries in the same vector space.

\subsubsection{Topic Label Generation}\label{sec: tag_assignment}
\textbf{TODO: shorten}
Later in the preprocessing pipeline, we will use a hierarchical clustering algorithm to cluster the documents and keywords (described in~\autoref{sec: clustering}). 
The algorithm outputs a hierarchy, where leaf nodes are documents or keywords and internal nodes are clusters.
The clustering result faces a similar problem with embedding-based approaches, that the clusters are not interpretable.
To address this issue, we use LLMs to assign human-like labels to each cluster.
Our approach is similar to a recent work done by Raval et al.~\cite{raval2023explainandtrust}, where they use LLMs to generate explanations for a user-selected set of points.
However, our task is more complicated because the prompt needs to (1) consider the hierarchical information, and (2) be able to process large clusters containing thousands of documents without breaking the token limit.

We describe our topic label generation process in a bottom-up manner:
(1) At the bottom level (one level above the leaf nodes), assign topic labels to the clusters using the document summaries;
(2) At any level above, assign labels to the clusters using the labels of their children and randomly sampled document summaries.

The first step is straightforward, where we simply use the document to generate labels for each cluster.
The bottom-level clusters usually contain only a few documents, so the token limit is unlikely to be exceeded.
We insert the summarization generated in~\autoref{sec: summarization} into the prompt template.
The prompt instructs the LLM to generate a label composed of a single noun phrase for the given documents.

Then at any level above, the cluster size increases dramatically and the token limit is likely to be exceeded.
To solve this problem, we first insert the labels of a cluster's children instead of the document.
This guarantees that the token limit will not be exceeded.
In early experiments, we found that the children's labels alone were not enough to generate a meaningful label for the parent cluster.
Therefore, we also insert documents randomly sampled from the cluster.
We enforce that each child has at least one article being sampled, and distribute the remaining token space proportionally to the size of each child.

\subsection{Models}
\subsubsection{Hypergraph Construction}
A hypergraph is a generalization of a graph in which an edge can connect any number of nodes~\cite{fischer2021hypergraphsurvey}.
A hyperedge thus represents a multi-way relationship between nodes.
In our work, we model two types of hypergraphs: document hypergraph and keyword hypergraph.
In a document hypergraph, the nodes are documents and the hyperedges are keywords.
Similarly, in a keyword hypergraph, the nodes are keywords and the hyperedges are documents.
Analyzing the document hypergraph and keyword hypergraph correspond to topic-based and entity-based analysis, respectively.
By modeling the corpus as hypergraphs, the system supports users to conduct both types of analysis simultaneously under a unified framework (\textbf{DC1}).
Below, we explain the relation between these two types of hypergraphs and how they are constructed.

Following the definition of a hypergraph, a hyperedge can be used to represent two types of multi-way relationships:
In a document hypergraph, a hyperedge (connecting document nodes) can be constructed between documents that mention the same keyword. 
In this case, the hyperedge represents the co-mention of a keyword.
In a keyword hypergraph, a hyperedge (connecting keyword nodes) can be constructed between keywords if they are mentioned together in the same document.
In this case, the hyperedge represents a co-occurrence relationship between keywords.
This means the two hypergraphs are both built from the same corpus, using the same relationship information, but from different perspectives.
Once the two hypergraphs are constructed, we further separately cluster them hierarchically.
Clusters in the document hypergraph represent topics that are discussed in the dataset.
Clusters in the keyword hypergraph represent keywords (entities or concepts) that are similar to each other.
For better interpretability of the clustering result, we further assign \textit{labels} to each cluster, as explained in~\autoref{sec: tag_assignment}.

Although these two types of hypergraphs are constructed from different perspectives, we utilize the \textit{dual} of a hypergraph to simplify the construction process.
The dual of a hypergraph is simply another hypergraph, where the nodes and hyperedges are interchanged, as shown in~\autoref{fig: duality}.
Using the duality feature, we first model the documents as nodes and keywords as hyperedges to construct the document hypergraph $H_D$.
The keyword hypergraph $H_C$ can then be easily constructed by taking the dual of $H_D$.
This construction process also allows us to map user interactions on documents and keywords to operations on a single hypergraph (\textbf{DC1}).
Even better, the duality feature between documents and keywords matches the user's mental model (\textbf{DC3}), making the visualization and interaction intuitive.

\begin{figure}
 \centering % avoid the use of \begin{center}...\end{center} and use \centering instead (more compact)
 \includegraphics[width=\columnwidth]{duality}
%  \captionsetup{belowskip=-14pt,aboveskip=3pt}
 \caption{Illustration of the dual of a hypergraph. 
 Left: a hypergraph with 4 nodes $v_1, v_2, v_3, v_4$ and 3 hyperedges $e_1=(v_1, v_2, v_3), e_2=(v_2, v_3), e_3=(v_3, v_4)$. 
 Right: the dual of the hypergraph, where the nodes and hyperedges interchanged.
 Nodes in the dual hypergraph are now $e_1, e_2, e_3$ and hyperedges are $v_1=(e_1), v_2=(e_1, e_2), v_3=(e_1, e_3), v_4=(e_2, e_3)$.
 }

\label{fig: duality}
\end{figure}

\subsubsection{Hierarchical Clustering}\label{sec: clustering}
\textbf{TODO: Move existing works to supplemental material}
Common clustering algorithms for graphs consider only graph connectivity.
However, for the best interpretability of the clustering result, the node embeddings must be also used in the clustering process.
This limits our choice of clustering algorithms to attributed node clustering algorithms.
Although there are existing approaches that can cluster attributed nodes on graphs such as EVA~\cite{citraro2020eva} and iLouvain~\cite{combe2015louvain}, they are not designed for hypergraphs.
In general, hypergraphs can be clustered in two different ways: 
(1) Directly operate on the hyperedges by generalizing the graph clustering algorithms.
For example, KamiÅ„ski et al.~\cite{kaminski2021hgraphcommunity} generalizes the modularity metric for graphs to hypergraphs; 
(2) First transform the hypergraph into a graph and then apply normal graph clustering algorithms~\cite{kumar2020new}.
Although the first approach is more intuitive, it is less scalable and hard to incorporate node attributes.
Thus, we have decided to design our clustering algorithm following the second approach.

Considering all the above, we implemented our hierarchical clustering algorithm by first transforming the hypergraph into a graph following the edge re-weighting process proposed by Kumar et al.~\cite{kumar2020new}.
Then an agglomerative clustering algorithm~\cite{steinbach2000doccluster} is applied on the re-weighted graph.
In agglomerative clustering, the key is to define node similarity and cluster similarity.
We can easily incorporate node attributes into the clustering process by defining the similarity between nodes and clusters as a weighted sum of attribute similarity $S_s$ and connectivity similarity $S_c$.
Since we're dealing with texts, we refer to the attribute similarity between nodes as semantic similarity. 

The semantic similarity $S_s(i, j)$ is the cosine similarity of the embeddings (dense vectors) of the two nodes, denoted as $vec_i$ and $vec_j$.
The connectivity similarity $S_c$ is the weighted Topological Overlap (wTO)~\cite{gysi2018wto},
which is a weighted generalization of the Overlap Coefficient~\cite{vijaymeena2016survey}, as shown in~\autoref{eq: connectivity_semantic_similarity}.
\begin{equation}\label{eq: connectivity_semantic_similarity}
    \mathbf{S_s}(i, j) = \frac{vec_i \cdot vec_j}{||vec_i|| \cdot ||vec_j||}, \quad
    \mathbf{S_c}(i, j) = \frac{\sum_{u=1}^N{w_{i,u}w_{u_j}} + w_{i,j}}{\min(k_i, k_j) + 1 - |w_{i,j}|}
\end{equation}
where $k_i = \sum_{j=1}^N |w_{i,j}|$ is the total weight of the edges connected to node $i$.
Finally, a weighting factor $\alpha$ is used to balance the two similarities, as shown in~\autoref{eq: similarity}.
\begin{equation}\label{eq: similarity}
    \mathbf{S} = \alpha \mathbf{S_s} + (1-\alpha) \mathbf{S_c}
\end{equation}
For the similarity between clusters, we used centroid similarity, i.e.\ the similarity between two clusters is the similarity between the centroids of the two clusters.
The algorithm takes a hypergraph $H=(V, E)$ and the embeddings of each node $Vec$ as input, and outputs a sequence of partitions $P=P_1, P_2, \dots P_k$.
Each partition corresponds to a level in the hierarchy, as shown in Algorithm 1.
\begin{algorithm}\label{alg: clustering}
    \caption{Agglomerative Clustering }\label{alg:cap}
    \hspace*{\algorithmicindent} \textbf{Input}: $H=(V, E)$, $Vec=\{vec_i |\, i \in V\}$ \\
    \hspace*{\algorithmicindent} \textbf{Output}: Sequence of partitions $\mathbf{P}=P_1, P_2, \dots P_k$ \\
    \begin{algorithmic}[1]
    \While{$|V| > 1$}
        \State$P_k = init\_partition(V)$ \Comment{Initialize each node as a cluster}
        \State$\mathbf{S_s} = cosine\_similarity(V\times V, Vec)$ 
        \State$\mathbf{S_c} = wTO(V\times V, E)$ \
        \For{$i \in V$}
            \State$j = most\_similar\_node(i, S_s, S_c)$ 
            \State$P_k = merge\_clusters(i, j)$ \Comment{Merge the two clusters}
        \EndFor
        \State$H^\prime=(V^\prime, E^\prime) = construct\_hypergraph(P_k)$ \Comment{clusters are the new nodes}
        \State$Vec^\prime = centroid\_similarity(V^\prime)$
        \State$V=V^\prime, E=E^\prime, Vec=Vec^\prime$ \Comment{Update for next iteration}
    \EndWhile
    \end{algorithmic}
\end{algorithm}


