{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2147483647"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import jsonlines\n",
    "from flask import Flask, redirect, render_template, request, url_for\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import openai\n",
    "csv.field_size_limit(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles1_csv = csv.DictReader(open(\"D:/Projects/Test/All the News/data/articles1.csv\"))\n",
    "articles2_csv = csv.DictReader(open(\"D:/Projects/Test/All the News/data/articles2.csv\"))\n",
    "articles3_csv = csv.DictReader(open(\"D:/Projects/Test/All the News/data/articles3.csv\"))\n",
    "dataset = [datum for datum in articles1_csv] + [datum for datum in articles2_csv] + [datum for datum in articles3_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362 3628\n",
      "1610 16100\n",
      "410 4104\n",
      "459 4596\n",
      "515 5150\n",
      "432 4322\n",
      "248 2489\n",
      "325 3254\n",
      "419 4190\n",
      "898 8985\n",
      "428 4284\n",
      "764 7647\n",
      "616 6165\n",
      "325 3257\n",
      "723 7234\n"
     ]
    }
   ],
   "source": [
    "# clean AllTheNews\n",
    "dataset_2016 = [datum for datum in dataset if datum['year'] == '2016.0']\n",
    "#dataset_2016 = dataset_2016[0:20]\n",
    "articles_grouped = defaultdict(list)\n",
    "for datum in dataset_2016:\n",
    "    articles_grouped[datum['publication']].append(datum)\n",
    "random_selected_dataset = []\n",
    "for publication, articles in articles_grouped.items():\n",
    "    total_articles = len(articles)\n",
    "    random_samples = random.sample(articles, math.floor(total_articles/10))\n",
    "    print(len(random_samples), total_articles)\n",
    "    random_selected_dataset += random_samples\n",
    "random_selected_dataset = random_selected_dataset[0:20]\n",
    "save_json(random_selected_dataset, r'D:/Projects/Test/All the News/data/processed_data/2016_10p.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    completions = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature = 0.0,\n",
    "        messages=messages)\n",
    "    gpt_response = completions['choices'][0]['message']['content'].strip() \n",
    "    return gpt_response   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(datum_sentences):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in datum_sentences] # merge the words into sentences\n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments(article, model=\"gpt-3.5-turbo-0613\"):\n",
    "    messages = [ \n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are an extraction system that extracts the main characters of a news article.\n",
    "                The main characters can be any organization, person or location that are heavily involved in the event described by the news article.\n",
    "                The user will provide you with a news article to extract.\n",
    "                Reply in the format '[character 1] [character 2]...'\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": article\n",
    "        } \n",
    "    ]\n",
    "    arguments = call_gpt(messages)\n",
    "    return arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events(sentence):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are an event extraction system. Please extract the events from user provided sentence.\n",
    "                An 'event' should contain one or more 'participants', which are the major participants in the event,\n",
    "                and a 'trigger', which is a verb that describes what happens between the participants.\n",
    "                The triggers and participants should be human-readable.\n",
    "                Reply with each line being an event in the format:\n",
    "                [trigger1], [participant 1], [participant 2], ...\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"Trump's inability to work with people beyond his base, as demonstrated by his comparison to Saddam Hussein's Iraq, is a major problem for the United States, as it requires the president to build bridges and form alliances in order to get things done.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"Problem, Trump, United States; \\n Inable, Trump, work with, people beyond his base; \\n Compare, Trump, Saddam Hussein's Iraq; \\n Require, president, build bridges and form alliances;\"},\n",
    "        { \"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events1(sentence,event_role):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are an event extraction system. Your task is to extract events from news articles provided by the user. \n",
    "                An 'event' should contain one or more 'participants', which are the major participants in the event,\n",
    "                and a 'trigger' is a verb that describes what kind of event happens between the participants. There are categories of events, and each trigger recognized is classified as one of the categories.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\" The user will provide a dictionary defining 'event category' as 'keys' and the 'types of participants' that can be involved in that event by 'values' of those keys.\n",
    "                The format of the input dictionary will be like \"['event type 1':['participant type 1','participant type 2', ...]], ['event type 2':['participant type 1','participant type 2', ...]], ...\".\n",
    "                Use this input to classify triggers and participants. \n",
    "                Classify the triggers into one of the categories specified by the dictionary strictly, if none of them apply answer unknown. \n",
    "                Also classify the type of participants strictly according to the user input.\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"user\", \"content\": f\"This is the input dictionary:{event_role}\"},\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"Now extract the triggers involved in the articles provided by the user and classify them into their event types.\n",
    "                The triggers and participants should be human-readable. \n",
    "                Reply with each line being an event in the format:\n",
    "                [event type 1 - trigger 1],[participant type 1 : participant 1], [participant type 2 : participant 2], ...\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"The fixture anticipated by fans as the marquee game of the second day of Euro 2016 ended in a 1 - 1 tie . In chaotic scenes involving hundreds of fans throngs of rival supporters rushed at one another hurling bottles chairs and other objects and forcing police in riot gear to fire tear gas in response . Fans skirmish ahead of the match in the French port city of Marseille on Saturday . Distressing footage emerged of men kicking and stomping on another person lying on the street in broad daylight as the rivalry turned violent prior to kick - off . Then at the end of the game Russian fans charged at the section of the stadium containing England supporters and more chaos ensued.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"[conflict:attack - hurling], [attacker-fans],[target-fans],[instrument- bottles chairs and other objects],[place- Marseille]; \\n [conflict:attack-fire] , [attacker-police],[target-fans],[instrument-tear gas],[place- Marseille];\"},\n",
    "        { \"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_sentence(article, arguments, model=\"gpt-3.5-turbo-0613\"):\n",
    "    messages = [ \n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are an summarization system that summarizes the events that happened between the main characters of a news article.\n",
    "                The user will provide you with a list of main characters and a news article to summarize.\n",
    "                Try to summarize the article with no more than three sentences. \n",
    "                Reply starts with 'The article discussed ...'\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": \"Main Characters:\\n{arguments} \\n\\n\\n Article: {article}\".format(arguments=arguments, article=article)\n",
    "        } \n",
    "    ]\n",
    "    sentence = call_gpt(messages)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_sentence(sentence):\n",
    "    if sentence.startswith('The article discussed how'):\n",
    "        stripped_sentence = sentence.replace('The article discussed how', '').strip()\n",
    "    elif sentence.startswith('The article discussed'):\n",
    "        stripped_sentence = sentence.replace('The article discussed', '').strip()\n",
    "    else:\n",
    "        print(\"!!!\")\n",
    "    return stripped_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20\n",
      "2/20\n",
      "3/20\n",
      "4/20\n",
      "5/20\n",
      "6/20\n",
      "7/20\n",
      "8/20\n",
      "9/20\n",
      "10/20\n",
      "11/20\n",
      "12/20\n",
      "13/20\n",
      "14/20\n",
      "15/20\n",
      "16/20\n",
      "17/20\n",
      "18/20\n",
      "19/20\n",
      "20/20\n"
     ]
    }
   ],
   "source": [
    "dataset = json.load(open(r'D:/Projects/Test/All the News/data/processed_data/2016_10p.json'))\n",
    "saved_dataset = []\n",
    "count = 0\n",
    "for datum in dataset:\n",
    "    try:\n",
    "        count += 1\n",
    "        #if count == 6: break\n",
    "        print(\"{}/{}\".format(count, len(dataset)))\n",
    "        saved_datum = {}\n",
    "        article = datum['content']\n",
    "        arguments = get_arguments(article)\n",
    "        sentence = summarize_sentence(article, arguments)\n",
    "        saved_datum['id'] = datum['id']\n",
    "        saved_datum['content'] = datum['content']\n",
    "        saved_datum['title'] = datum['title']\n",
    "        saved_datum['publication'] = datum['publication']\n",
    "        saved_datum['author'] = datum['author']\n",
    "        saved_datum['url'] = datum['url']\n",
    "        saved_datum['date'] = datum['date']\n",
    "        saved_datum['summary'] = sentence\n",
    "        saved_dataset.append(saved_datum)\n",
    "    except:\n",
    "        continue\n",
    "save_json(saved_dataset, r'D:/Projects/Test/All the News/data/summarized/summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('D:/Projects/OneIE/resource/valid_patterns_rams/event_role.json')\n",
    "event_role= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20\n",
      "1/20\n",
      "2/20\n",
      "3/20\n",
      "4/20\n",
      "5/20\n",
      "6/20\n",
      "7/20\n",
      "8/20\n",
      "9/20\n",
      "10/20\n",
      "11/20\n",
      "12/20\n",
      "13/20\n",
      "14/20\n",
      "15/20\n",
      "16/20\n",
      "17/20\n",
      "18/20\n",
      "19/20\n"
     ]
    }
   ],
   "source": [
    "# AllTheNews\n",
    "AllTheNews_summarized = json.load(open(r'D:/Projects/Test/All the News/data/summarized/summary.json'))\n",
    "res_events = []\n",
    "error_datum = []\n",
    "for index, datum in enumerate(AllTheNews_summarized):\n",
    "    print('{}/{}'.format(index, len(AllTheNews_summarized)))\n",
    "    sentence = strip_sentence(datum['summary'])\n",
    "    events = extract_events1(sentence,event_role)\n",
    "    datum['events'] = events\n",
    "    res_events.append(datum)\n",
    "save_json(res_events, r'D:/Projects/Test/All the News/events/events1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def post_process_events(dataset):\n",
    "    for index, datum in enumerate(dataset):\n",
    "        datum['doc_id'] = index\n",
    "        # datum['id']=datum['id']\n",
    "        # datum['content']=datum['content']\n",
    "        # datum['title']=datum['title']\n",
    "        # datum['publication']=datum['publication']\n",
    "        # datum['author']=datum['author']\n",
    "        # datum['date']=datum['date']\n",
    "        datum['events_raw'] = datum['events']\n",
    "        events_str = datum['events'].split('\\n')\n",
    "        events = []\n",
    "        for event_str in events_str:\n",
    "            arguments=[]\n",
    "            event_str=event_str[1:-1]\n",
    "            components = event_str.split(',')\n",
    "            event_type_raw = components[0].split('-')\n",
    "            event_type=event_type_raw[0].strip()\n",
    "            trigger_raw = event_type_raw[1].split(',')\n",
    "            trigger = trigger_raw[0].strip()\n",
    "            trigger = trigger[0:-1]\n",
    "            arguments_raw = [arg.strip().strip(punctuation) for arg in components[1:]]\n",
    "            for dat in arguments_raw:\n",
    "                temp = dat.split(',')\n",
    "                for arg in temp:\n",
    "                    arg_raw=arg.split('-')\n",
    "                    arg_type=arg_raw[0].strip()\n",
    "                    args = arg_raw[1:]\n",
    "                    args_final=' '.join([str(elem) for elem in args]).strip()\n",
    "                    arguments.append({arg_type:args_final})\n",
    "            events.append({'event_type':event_type,'trigger': trigger, 'arguments': arguments})\n",
    "        datum['events'] = events\n",
    "    return dataset\n",
    "\n",
    "dataset = json.load(open(r'D:/Projects/Test/All the News/events/events1.json'))\n",
    "processed_dataset = post_process_events(dataset)\n",
    "save_json(processed_dataset, r'D:/Projects/Test/All the News/result/result.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
