{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import jsonlines\n",
    "from flask import Flask, redirect, render_template, request, url_for\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import openai\n",
    "import re\n",
    "import pandas as pd\n",
    "#csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = csv.DictReader(open(\"D:\\Projects\\Test\\IEEE_papers\\Raw_data\\IEEE VIS papers 1990-2022 - Main dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [datum for datum in data]\n",
    "dataset = dataset[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(dataset, r'D:/Projects/Test/IEEE_papers/processed_data/processed_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    completions = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature = 0.0,\n",
    "        messages=messages)\n",
    "    gpt_response = completions['choices'][0]['message']['content'].strip() \n",
    "    return gpt_response   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(datum_sentences):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in datum_sentences] # merge the words into sentences\n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events(sentence):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a state of the art event extraction system. \n",
    "                Your task is to extract only the most important event that describes the main research idea from Research paper abstracts. \n",
    "                Strictly extract only one event. This event should be the major research focus of the abstract.\n",
    "                The events should be human-readable. \n",
    "                Reply in JSON format with each line being an event in the format:\n",
    "                [event];\n",
    "                The abstract of research papers will be provided by the user.\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"We present KiriPhys, a new type of data physicalization based on kirigami, a traditional Japanese art form that uses paper-cutting. Within the kirigami possibilities, we investigate how different aspects of cutting patterns offer opportunities for mapping data to both independent and dependent physical variables. As a first step towards understanding the data physicalization opportunities in KiriPhys, we conducted a qualitative study in which 12 participants interacted with four KiriPhys examples. Our observations of how people interact with, understand, and respond to KiriPhys suggest that KiriPhys: 1) provides new opportunities for interactive, layered data exploration, 2) introduces elastic expansion as a new sensation that can reveal data, and 3) offers data mapping possibilities while providing a pleasurable experience that stimulates curiosity and engagement.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"[data physicalization based on kirigami];\"},       \n",
    "        { \"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged(sentence,cl):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a named entity recognition model. You will be given an abstract of a research paper and the event recognized in that abstract by the user.\n",
    "                The format of the input defining event in research paper will be:\n",
    "                [event];\n",
    "                Extract the 2 main participants in the event. A participant can be a proposed solution, product, algorithm, technique, instruments, tools, processes or phenomena.  \n",
    "                The number of main participants should be 2 or less strictly. \n",
    "                Ignore any other participants other than the two main participants.\n",
    "                Reply in JSON format with each line being an event in the format:\n",
    "                [event]:[main participant 1],[main participant 2];\n",
    "            \"\"\"\n",
    "        },   \n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"We present KiriPhys, a new type of data physicalization based on kirigami, a traditional Japanese art form that uses paper-cutting. Within the kirigami possibilities, we investigate how different aspects of cutting patterns offer opportunities for mapping data to both independent and dependent physical variables. As a first step towards understanding the data physicalization opportunities in KiriPhys, we conducted a qualitative study in which 12 participants interacted with four KiriPhys examples. Our observations of how people interact with, understand, and respond to KiriPhys suggest that KiriPhys: 1) provides new opportunities for interactive, layered data exploration, 2) introduces elastic expansion as a new sensation that can reveal data, and 3) offers data mapping possibilities while providing a pleasurable experience that stimulates curiosity and engagement.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"[data physicalization based on kirigami],[Kirigami],[KiriPhys];\"},    \n",
    "        { \"role\": \"user\", \"content\": f\"This is the input defining the extracted event :{cl}\"},  \n",
    "        { \"role\": \"user\", \"content\": f\"This is the news article:{sentence}\"},\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_all(sentence,cl):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You will be given an abstract of a research paper and the event graph of one event in that article by the user.\n",
    "                The format of the event graph will be:\n",
    "                [event]:[main participant 1],[main participant 2];\n",
    "                Arguments refer to the specific entities, participants, or elements that play specific roles in an event.\n",
    "                Extract the main arguments and their roles involved in each event.\n",
    "                Strictly assign an argument type to each argument.                \n",
    "                Reply in JSON format with each line being an event in the format:\n",
    "                [event]:[main participant 1],[main participant 2]:[argument type 1 - argument 1],[argument type 2 - argument 2],...; \n",
    "            \"\"\"\n",
    "        },   \n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"We present KiriPhys, a new type of data physicalization based on kirigami, a traditional Japanese art form that uses paper-cutting. Within the kirigami possibilities, we investigate how different aspects of cutting patterns offer opportunities for mapping data to both independent and dependent physical variables. As a first step towards understanding the data physicalization opportunities in KiriPhys, we conducted a qualitative study in which 12 participants interacted with four KiriPhys examples. Our observations of how people interact with, understand, and respond to KiriPhys suggest that KiriPhys: 1) provides new opportunities for interactive, layered data exploration, 2) introduces elastic expansion as a new sensation that can reveal data, and 3) offers data mapping possibilities while providing a pleasurable experience that stimulates curiosity and engagement.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"[data physicalization based on kirigami]:[Kirigami],[KiriPhys]:[type of data physicalization - KiriPhys],[art form - Kirigami];\"},   \n",
    "        { \"role\": \"user\", \"content\": f\"This is the input defining the extracted event and the main participants, strictly use only this event for further tasks  :{cl}\"},  \n",
    "        { \"role\": \"user\", \"content\": f\"This is the news article:{sentence}\"},\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events\n",
    "                # For each event assign roles that the main participants play.\n",
    "                # Only assign roles to the main participants given by user input. \n",
    "                # These should be the participants heavily involved in the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_sentence(sentence):\n",
    "    if sentence.startswith('The article discussed how'):\n",
    "        stripped_sentence = sentence.replace('The article discussed how', '').strip()\n",
    "        stripped_sentence = re.sub(\",\",\"\",stripped_sentence)\n",
    "    elif sentence.startswith('The article discussed'):\n",
    "        stripped_sentence = sentence.replace('The article discussed', '').strip()\n",
    "        stripped_sentence = re.sub(\",\",\"\",stripped_sentence)\n",
    "    else:\n",
    "        print(\"!!!\")\n",
    "    return stripped_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentences'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8196\\52629131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdatum\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msaved_datum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0marguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[1;31m#  sentence = summarize_sentence(article, arguments)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentences'"
     ]
    }
   ],
   "source": [
    "# RAMS\n",
    "saved_dataset = []\n",
    "for datum in dataset:\n",
    "    saved_datum = {}\n",
    "    article = merge_sentences(datum['sentences'])\n",
    "    arguments = (article)\n",
    "  #  sentence = summarize_sentence(article, arguments)\n",
    "  #  print(sentence)\n",
    "    # saved_datum['content'] = datum['sentences']\n",
    "    # saved_datum['url'] = datum['source_url']\n",
    "    # saved_datum['summary'] = sentence\n",
    "    saved_dataset.append(arguments)\n",
    "save_json(saved_dataset, r'D:/Projects/Test/summarized/args.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20\n",
      "1/20\n",
      "2/20\n",
      "3/20\n",
      "4/20\n",
      "5/20\n",
      "6/20\n",
      "7/20\n",
      "8/20\n",
      "9/20\n",
      "10/20\n",
      "11/20\n",
      "12/20\n",
      "13/20\n",
      "14/20\n",
      "15/20\n",
      "16/20\n",
      "17/20\n",
      "18/20\n",
      "19/20\n"
     ]
    }
   ],
   "source": [
    "dat = json.load(open(r'../IEEE_papers/processed_data/processed_data.json'))\n",
    "# cl = json.load(open(r'D:/Projects/Test/All the News/events/merged_bb.json'))\n",
    "#ar = json.load(open(r'D:/Projects/Test/All the News/events/cl_ents.json'))\n",
    "res_events = []\n",
    "error_datum = []\n",
    "# sentence = \"The article discussed the extensive history of doping in Russia, dating back to the 1983 Soviet Union's detailed instructions to inject top athletes with anabolic steroids in order to ensure dominance at the Los Angeles Olympics. Dr. Sergei Portugalov, a key figure in Russia's current doping scandal, was named as the mastermind behind the doping program. The revelations of these schemes led to the banning of Russia's track and field team from the Rio Games, the most severe doping penalty in Olympic history.\"\n",
    "for index, datum in enumerate(dat):\n",
    "    print('{}/{}'.format(index, len(dat)))\n",
    "    sentence = datum['Abstract'].strip()\n",
    "    sentence = re.sub(\"-\",\" \",sentence)\n",
    "    cl = extract_events(sentence)\n",
    "    # cl = re.sub(\":\",\"\",cl)\n",
    "    # cl = re.sub(\"-\",\"\",cl)\n",
    "    events = merged(sentence,cl)\n",
    "    # print(events)\n",
    "    ev = merged_all(sentence,events)\n",
    "    datum['events'] = ev\n",
    "    # print(ev)\n",
    "    res_events.append(datum)\n",
    "save_json(res_events, r'../IEEE_papers/Events/events_merged_final.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20\n",
      "1/20\n",
      "2/20\n",
      "3/20\n",
      "4/20\n",
      "5/20\n",
      "6/20\n",
      "7/20\n",
      "8/20\n",
      "9/20\n",
      "10/20\n",
      "11/20\n",
      "12/20\n",
      "13/20\n",
      "14/20\n",
      "15/20\n",
      "16/20\n",
      "17/20\n",
      "18/20\n",
      "19/20\n"
     ]
    }
   ],
   "source": [
    "AllTheNews_summarized = json.load(open(r'D:/Projects/Test/All the News/data/summarized/summary.json'))\n",
    "# cl = json.load(open(r'D:/Projects/Test/All the News/events/classes.json'))\n",
    "res_events = []\n",
    "error_datum = []\n",
    "for index, datum in enumerate(AllTheNews_summarized):\n",
    "    print('{}/{}'.format(index, len(AllTheNews_summarized)))\n",
    "    sentence = strip_sentence(datum['summary'])\n",
    "    cl = extract_cl(sentence)\n",
    "    events = merged(sentence,cl)\n",
    "    # datum['events'] = events\n",
    "    res_events.append(events)\n",
    "save_json(res_events, r'D:/Projects/Test/All the News/events/ev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def post_process_events(dataset):\n",
    "    for index, datum in enumerate(dataset):\n",
    "        datum['doc_id'] = index\n",
    "        datum['events_raw'] = datum['events']\n",
    "        events_str = datum['events'].split('\\n')\n",
    "        events = []\n",
    "        for event_str in events_str:\n",
    "            arguments=[]\n",
    "            main_characters = []\n",
    "            event_str = event_str.strip()\n",
    "            components = event_str.split(':')\n",
    "            event_type = components[0].strip()\n",
    "            # event_type_p=event_type_raw[0].split('-')\n",
    "            # event_type = event_type_raw[0][1:]\n",
    "            # trigger = event_type_raw[1:2]\n",
    "            # trigger=' '.join([str(elem) for elem in trigger]).strip().strip(punctuation)\n",
    "            # if(trigger==\"\"):\n",
    "            #     trigger=event_type\n",
    "            chars = components[1].split('],[')\n",
    "            # arguments_raw = [arg.strip().strip(punctuation) for arg in components[1:]]\n",
    "            arguments_raw = components[2:]\n",
    "            # print(arguments_raw)\n",
    "            # chars = re.sub(\",\",\"\",chars)\n",
    "            for dat in chars:\n",
    "                dat=re.sub(\",\",\"\",dat)\n",
    "                dat = str(dat).replace('[', '').replace(']', '')\n",
    "                # print(dat)\n",
    "                main_characters.append(dat)\n",
    "            for args in arguments_raw:\n",
    "                # args = re.sub(\",\",\"\",args)\n",
    "                temp = args.split('],[')\n",
    "                for arg in temp:\n",
    "                    # print(arg)\n",
    "                    # arg=re.sub(\",\",\"\",arg)\n",
    "                    arg_raw=arg.split('-')\n",
    "                    arg_type=arg_raw[0].strip()\n",
    "                    # arg_type=re.sub(\",\",\"\",arg_type)\n",
    "                    args = arg_raw[1:]\n",
    "                    args_final=' '.join([str(elem) for elem in args]).strip().strip(punctuation)\n",
    "                    # args_final=re.sub(\",\",\"\",args_final)\n",
    "                    arg_type = str(arg_type).replace('[', '').replace(']', '')\n",
    "                    arguments.append({arg_type:args_final})\n",
    "            events.append({'Trigger':event_type, 'Main Participants': main_characters, 'Arguments': arguments})\n",
    "        datum['events'] = events\n",
    "    return dataset\n",
    "\n",
    "dataset = json.load(open(r'../IEEE_papers/Events/events_merged_final.json'))\n",
    "processed_dataset = post_process_events(dataset)\n",
    "save_json(processed_dataset, r'../IEEE_papers/Result/events_merged_final.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
