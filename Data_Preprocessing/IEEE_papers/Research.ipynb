{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import jsonlines\n",
    "from flask import Flask, redirect, render_template, request, url_for\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import openai\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "#csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['openai_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = csv.DictReader(open(\"../IEEE_papers/Raw_data/IEEE VIS papers 1990-2022 - Main dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [datum for datum in data]\n",
    "# dataset = dataset[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(dataset, r'../IEEE_papers/processed_data/processed_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    completions = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature = 0.0,\n",
    "        messages=messages)\n",
    "    gpt_response = completions['choices'][0]['message']['content'].strip() \n",
    "    return gpt_response   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(datum_sentences):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in datum_sentences] # merge the words into sentences\n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events(sentence):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a state of the art event extraction system. \n",
    "                Your task is to extract only the most important event that describes the main research idea from Research paper abstract.\n",
    "                Strictly extract only one event. This event should be the major research focus of the abstract.\n",
    "                The events should be human-readable. \n",
    "                Reply in JSON format with each line being an event in the format:\n",
    "                [event];\n",
    "                The abstract of research papers will be provided by the user.\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"The success of DL can be attributed to hours of parameter and architecture tuning by human experts. Neural Architecture Search (NAS) techniques aim to solve this problem by automating the search procedure for DNN architectures making it possible for non-experts to work with DNNs. Specifically, One-shot NAS techniques have recently gained popularity as they are known to reduce the search time for NAS techniques. One-Shot NAS works by training a large template network through parameter sharing which includes all the candidate NNs. This is followed by applying a procedure to rank its components through evaluating the possible candidate architectures chosen randomly. However, as these search models become increasingly powerful and diverse, they become harder to understand. Consequently, even though the search results work well, it is hard to identify search biases and control the search progression, hence a need for explainability and human-in-the-loop (HIL) One-Shot NAS. To alleviate these problems, we present NAS-Navigator, a visual analytics (VA) system aiming to solve three problems with One-Shot NAS; explainability, HIL design, and performance improvements compared to existing state-of-the-art (SOTA) techniques. NAS-Navigator gives full control of NAS back in the hands of the users while still keeping the perks of automated search, thus assisting non-expert users. Analysts can use their domain knowledge aided by cues from the interface to guide the search. Evaluation results confirm the performance of our improved One-Shot NAS algorithm is comparable to other SOTA techniques. While adding Visual Analytics (VA) using NAS-Navigator shows further improvements in search time and performance. We designed our interface in collaboration with several deep learning researchers and evaluated NAS-Navigator through a control experiment and expert interviews.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"[automating the search procedure for DNN architectures using Neural Architecture Search (NAS) techniques];\"},         \n",
    "        { \"role\": \"user\", \"content\": f\"This is the research paper abstract:{sentence}\"},\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged(sentence,cl,keywords):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a named entity recognition model. You will be given an abstract of a research paper and the event recognized in that abstract by the user.\n",
    "                The format of the input defining event in research paper will be:\n",
    "                [event];\n",
    "                A list of key words in the abstract will be provided by the user.\n",
    "                These key words define the main concepts involved in the research paper. \n",
    "                Use these key words to find the 4 most important main participants.\n",
    "                A participant can be a concept, proposed solution, product, algorithm, technique, instruments, tools, processes or phenomena.\n",
    "                Strictly extract 4 or less main participants.\n",
    "                format of key words will be :\n",
    "                \"key word 1, key word 2...\"  \n",
    "                Reply in JSON format with each line being an event in the format:\n",
    "                [event]:[main participant 1],[main participant 2]...;\n",
    "            \"\"\"\n",
    "        },   \n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"The success of DL can be attributed to hours of parameter and architecture tuning by human experts. Neural Architecture Search (NAS) techniques aim to solve this problem by automating the search procedure for DNN architectures making it possible for non-experts to work with DNNs. Specifically, One-shot NAS techniques have recently gained popularity as they are known to reduce the search time for NAS techniques. One-Shot NAS works by training a large template network through parameter sharing which includes all the candidate NNs. This is followed by applying a procedure to rank its components through evaluating the possible candidate architectures chosen randomly. However, as these search models become increasingly powerful and diverse, they become harder to understand. Consequently, even though the search results work well, it is hard to identify search biases and control the search progression, hence a need for explainability and human-in-the-loop (HIL) One-Shot NAS. To alleviate these problems, we present NAS-Navigator, a visual analytics (VA) system aiming to solve three problems with One-Shot NAS; explainability, HIL design, and performance improvements compared to existing state-of-the-art (SOTA) techniques. NAS-Navigator gives full control of NAS back in the hands of the users while still keeping the perks of automated search, thus assisting non-expert users. Analysts can use their domain knowledge aided by cues from the interface to guide the search. Evaluation results confirm the performance of our improved One-Shot NAS algorithm is comparable to other SOTA techniques. While adding Visual Analytics (VA) using NAS-Navigator shows further improvements in search time and performance. We designed our interface in collaboration with several deep learning researchers and evaluated NAS-Navigator through a control experiment and expert interviews.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"[automating the search procedure for DNN architectures using Neural Architecture Search (NAS) techniques]:[Neural Architecture Search (NAS) techniques],[DNN architectures],[Visual Analytics],[Deep Learning];\"},\n",
    "        { \"role\": \"user\", \"content\": f\"This is the list of keywords in the research paper abstract:{keywords}\"},     \n",
    "        { \"role\": \"user\", \"content\": f\"This is the input defining the extracted event :{cl}\"},  \n",
    "        { \"role\": \"user\", \"content\": f\"This is the research paper abstract:{sentence}\"},\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_all(sentence,cl):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You will be given an abstract of a research paper and the event graph of one event in that article by the user.\n",
    "                The format of the event graph will be:\n",
    "                [event]:[main participant 1],[main participant 2];\n",
    "                Arguments refer to the specific entities, participants, or elements that play specific roles in an event.\n",
    "                Extract the main arguments and their roles involved in each event.\n",
    "                Each extracted argument has to play a major role in the event.\n",
    "                Strictly assign a role to each argument.                \n",
    "                Reply in JSON format with each line being an event in the format:\n",
    "                [event]:[main participant 1],[main participant 2]...:[role 1 - argument 1],[role 2 - argument 2],...; \n",
    "            \"\"\"\n",
    "        },   \n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\": \"The success of DL can be attributed to hours of parameter and architecture tuning by human experts. Neural Architecture Search (NAS) techniques aim to solve this problem by automating the search procedure for DNN architectures making it possible for non-experts to work with DNNs. Specifically, One-shot NAS techniques have recently gained popularity as they are known to reduce the search time for NAS techniques. One-Shot NAS works by training a large template network through parameter sharing which includes all the candidate NNs. This is followed by applying a procedure to rank its components through evaluating the possible candidate architectures chosen randomly. However, as these search models become increasingly powerful and diverse, they become harder to understand. Consequently, even though the search results work well, it is hard to identify search biases and control the search progression, hence a need for explainability and human-in-the-loop (HIL) One-Shot NAS. To alleviate these problems, we present NAS-Navigator, a visual analytics (VA) system aiming to solve three problems with One-Shot NAS; explainability, HIL design, and performance improvements compared to existing state-of-the-art (SOTA) techniques. NAS-Navigator gives full control of NAS back in the hands of the users while still keeping the perks of automated search, thus assisting non-expert users. Analysts can use their domain knowledge aided by cues from the interface to guide the search. Evaluation results confirm the performance of our improved One-Shot NAS algorithm is comparable to other SOTA techniques. While adding Visual Analytics (VA) using NAS-Navigator shows further improvements in search time and performance. We designed our interface in collaboration with several deep learning researchers and evaluated NAS-Navigator through a control experiment and expert interviews.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"[automating the search procedure for DNN architectures using Neural Architecture Search (NAS) techniques]:[Visual Analytics],[Deep Learning],[Neural Architecture Search (NAS) techniques],[DNN architectures]:[search procedure - Neural Architecture Search (NAS) techniques],[reducing search time - One Shot NAS techniques],[visual analytics system - NAS Navigator];\"},   \n",
    "        { \"role\": \"user\", \"content\": f\"This is the input defining the extracted event and the main participants, strictly use only this event for further tasks  :{cl}\"},  \n",
    "        { \"role\": \"user\", \"content\": f\"This is the news article:{sentence}\"},\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events\n",
    "                # For each event assign roles that the main participants play.\n",
    "                # Only assign roles to the main participants given by user input. \n",
    "                # These should be the participants heavily involved in the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_sentence(sentence):\n",
    "    if sentence.startswith('The article discussed how'):\n",
    "        stripped_sentence = sentence.replace('The article discussed how', '').strip()\n",
    "        stripped_sentence = re.sub(\",\",\"\",stripped_sentence)\n",
    "    elif sentence.startswith('The article discussed'):\n",
    "        stripped_sentence = sentence.replace('The article discussed', '').strip()\n",
    "        stripped_sentence = re.sub(\",\",\"\",stripped_sentence)\n",
    "    else:\n",
    "        print(\"!!!\")\n",
    "    return stripped_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20\n",
      "1/20\n",
      "2/20\n",
      "3/20\n",
      "4/20\n",
      "5/20\n",
      "6/20\n",
      "7/20\n",
      "8/20\n",
      "9/20\n",
      "10/20\n",
      "11/20\n",
      "12/20\n",
      "13/20\n",
      "14/20\n",
      "15/20\n",
      "16/20\n",
      "17/20\n",
      "18/20\n",
      "19/20\n"
     ]
    }
   ],
   "source": [
    "dat = json.load(open(r'../IEEE_papers/processed_data/processed_data.json'))\n",
    "res_events = []\n",
    "error_datum = []\n",
    "for index, datum in enumerate(dat):\n",
    "    print('{}/{}'.format(index, len(dat)))\n",
    "    sentence = datum['Abstract'].strip()\n",
    "    keywords = datum['AuthorKeywords'].strip()\n",
    "    sentence = re.sub(\"-\",\" \",sentence)\n",
    "    cl = extract_events(sentence)\n",
    "    events = merged(sentence,cl,keywords)\n",
    "    ev = merged_all(sentence,events)\n",
    "    datum['events'] = ev\n",
    "    res_events.append(datum)\n",
    "save_json(res_events, r'../IEEE_papers/Events/events_merged2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def post_process_events(dataset):\n",
    "    for index, datum in enumerate(dataset):\n",
    "        datum['doc_id'] = index\n",
    "        datum['events_raw'] = datum['events']\n",
    "        events_str = datum['events'].split('\\n')\n",
    "        events = []\n",
    "        for event_str in events_str:\n",
    "            arguments=[]\n",
    "            main_characters = []\n",
    "            event_str = event_str.strip()\n",
    "            components = event_str.split(':')\n",
    "            event_type = components[0].strip()\n",
    "            chars = components[1].split('],[')\n",
    "            arguments_raw = components[2:]\n",
    "            for dat in chars:\n",
    "                dat=re.sub(\",\",\"\",dat)\n",
    "                dat = str(dat).replace('[', '').replace(']', '')\n",
    "                main_characters.append(dat)\n",
    "            for args in arguments_raw:\n",
    "                temp = args.split('],[')\n",
    "                for arg in temp:\n",
    "                    arg_raw=arg.split(' - ')\n",
    "                    arg_type=arg_raw[0].strip()\n",
    "                    args = arg_raw[1:]\n",
    "                    args_final=' '.join([str(elem) for elem in args]).strip().strip(punctuation)\n",
    "                    arg_type = str(arg_type).replace('[', '').replace(']', '')\n",
    "                    arguments.append({arg_type:args_final})\n",
    "            events.append({'Trigger':event_type, 'Main Participants': main_characters, 'Arguments': arguments})\n",
    "        datum['events'] = events\n",
    "    return dataset\n",
    "\n",
    "dataset = json.load(open(r'../IEEE_papers/Events/events_merged2.json'))\n",
    "processed_dataset = post_process_events(dataset)\n",
    "save_json(processed_dataset, r'../IEEE_papers/Result/events_merged2.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
